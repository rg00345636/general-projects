{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n    #for filename in filenames:\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-16T15:11:47.405681Z","iopub.execute_input":"2023-08-16T15:11:47.406070Z","iopub.status.idle":"2023-08-16T15:11:47.412408Z","shell.execute_reply.started":"2023-08-16T15:11:47.406039Z","shell.execute_reply":"2023-08-16T15:11:47.411325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!git clone https://github.com//Tutorial-Book-Utils","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:11:47.450978Z","iopub.execute_input":"2023-08-16T15:11:47.451989Z","iopub.status.idle":"2023-08-16T15:11:47.456234Z","shell.execute_reply.started":"2023-08-16T15:11:47.451951Z","shell.execute_reply":"2023-08-16T15:11:47.455324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install gdown    ","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:11:47.499946Z","iopub.execute_input":"2023-08-16T15:11:47.500249Z","iopub.status.idle":"2023-08-16T15:11:55.218727Z","shell.execute_reply.started":"2023-08-16T15:11:47.500223Z","shell.execute_reply":"2023-08-16T15:11:55.217203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!python Tutorial-Book-Utils/PL_data_loader.py --data FaceMaskDetection","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:11:55.221739Z","iopub.execute_input":"2023-08-16T15:11:55.222176Z","iopub.status.idle":"2023-08-16T15:11:55.228277Z","shell.execute_reply.started":"2023-08-16T15:11:55.222134Z","shell.execute_reply":"2023-08-16T15:11:55.227080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade --no-cache-dir gdown","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:11:55.229970Z","iopub.execute_input":"2023-08-16T15:11:55.230644Z","iopub.status.idle":"2023-08-16T15:12:02.363642Z","shell.execute_reply.started":"2023-08-16T15:11:55.230610Z","shell.execute_reply":"2023-08-16T15:12:02.362441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!unzip -q Face\\ Mask\\ Detection.zip","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:02.367887Z","iopub.execute_input":"2023-08-16T15:12:02.368870Z","iopub.status.idle":"2023-08-16T15:12:02.373877Z","shell.execute_reply.started":"2023-08-16T15:12:02.368826Z","shell.execute_reply":"2023-08-16T15:12:02.372727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! cp -R /kaggle/input/tbx-11/* /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:02.375816Z","iopub.execute_input":"2023-08-16T15:12:02.376227Z","iopub.status.idle":"2023-08-16T15:12:04.154825Z","shell.execute_reply.started":"2023-08-16T15:12:02.376195Z","shell.execute_reply":"2023-08-16T15:12:04.153522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/input/bboxaggregated13aug","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:19:36.024038Z","iopub.execute_input":"2023-08-16T15:19:36.025180Z","iopub.status.idle":"2023-08-16T15:19:36.034731Z","shell.execute_reply.started":"2023-08-16T15:19:36.025135Z","shell.execute_reply":"2023-08-16T15:19:36.033632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/input/bboxdf-final-prebox\nmytraindf = pd.read_csv(\"bbox_df_train_finalbackupprocessed.csv\")\nmytraindf.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:24:57.451661Z","iopub.execute_input":"2023-08-16T15:24:57.452057Z","iopub.status.idle":"2023-08-16T15:24:57.513840Z","shell.execute_reply.started":"2023-08-16T15:24:57.452024Z","shell.execute_reply":"2023-08-16T15:24:57.512867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mytraindf_new = mytraindf[mytraindf['file_name'].str.contains('tb')] \nmytraindf_new\n\n#df = df[df['Credit-Rating'].str.contains('Fair')]","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:33:37.694936Z","iopub.execute_input":"2023-08-16T15:33:37.695317Z","iopub.status.idle":"2023-08-16T15:33:37.735820Z","shell.execute_reply.started":"2023-08-16T15:33:37.695283Z","shell.execute_reply":"2023-08-16T15:33:37.734761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.unique(mytraindf_new.newclass_name)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:33:53.038980Z","iopub.execute_input":"2023-08-16T15:33:53.039697Z","iopub.status.idle":"2023-08-16T15:33:53.047380Z","shell.execute_reply.started":"2023-08-16T15:33:53.039659Z","shell.execute_reply":"2023-08-16T15:33:53.046376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.unique(mytraindf.newclass_name)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:26:21.378315Z","iopub.execute_input":"2023-08-16T15:26:21.378716Z","iopub.status.idle":"2023-08-16T15:26:21.391666Z","shell.execute_reply.started":"2023-08-16T15:26:21.378686Z","shell.execute_reply":"2023-08-16T15:26:21.390608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.3  Bounding Box Diagram ","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport matplotlib.patches as patches\nfrom bs4 import BeautifulSoup\n\nimg_list = sorted(glob.glob('TBX11K/imgs/tb/*'))\nannot_list = sorted(glob.glob('TBX11K/annotations/xml/*'))\n\n#img_list = sorted(glob.glob('TBX11K/imgs/tb/*'))\n#annot_list = sorted(glob.glob('TBX11K/annotations/json*'))","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.157049Z","iopub.execute_input":"2023-08-16T15:12:04.157449Z","iopub.status.idle":"2023-08-16T15:12:04.164947Z","shell.execute_reply.started":"2023-08-16T15:12:04.157409Z","shell.execute_reply":"2023-08-16T15:12:04.163845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(img_list))\nprint(len(annot_list))","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.166778Z","iopub.execute_input":"2023-08-16T15:12:04.167222Z","iopub.status.idle":"2023-08-16T15:12:04.176185Z","shell.execute_reply.started":"2023-08-16T15:12:04.167187Z","shell.execute_reply":"2023-08-16T15:12:04.175134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(img_list[:10])\nprint(annot_list[:10])","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.177730Z","iopub.execute_input":"2023-08-16T15:12:04.178385Z","iopub.status.idle":"2023-08-16T15:12:04.186559Z","shell.execute_reply.started":"2023-08-16T15:12:04.178352Z","shell.execute_reply":"2023-08-16T15:12:04.185417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize Bounding boxes","metadata":{}},{"cell_type":"code","source":"def generate_box(obj, width, height):\n    width_ratio = 512 / float(width)\n    height_ratio = 512 / float(height)\n    \n    xmin = float(obj.find('xmin').text)\n    ymin = float(obj.find('ymin').text)\n    xmax = float(obj.find('xmax').text)\n    ymax = float(obj.find('ymax').text)\n    \n    true_xmin = xmin * width_ratio\n    true_ymin = ymin * height_ratio\n    true_xmax = true_xmin + ((xmax - xmin) * width_ratio)\n    true_ymax = true_ymin + ((ymax - ymin) * height_ratio)\n\n    return [true_xmin, true_ymin, true_xmax, true_ymax]\n\ndef generate_label(obj):\n    if obj.find('name').text == \"ActiveTuberculosis\":\n        return 1\n    elif obj.find('name').text == \"ObsoletePulmonaryTuberculosis\":\n        return 2\n    return 0\n\ndef generate_target(file): \n    with open(file) as f:\n        data = f.read()\n        soup = BeautifulSoup(data, \"html.parser\")\n        width_element = soup.find('width')\n        if width_element:\n            width = width_element.get_text()\n            print(f\"width text: {width}\")\n        else:\n            width = 1811\n            print(\"width not found\")\n\n        height_element = soup.find('height')\n        if height_element:\n            height = height_element.get_text()\n            print(f\"height text: {height}\")\n        else:\n            height = 2022\n            print(\"height not found\")\n    \n        objects = soup.find_all(\"object\")\n        num_objs = len(objects)\n\n        boxes = []\n        labels = []\n        for i in objects:\n            boxes.append(generate_box(i, width, height))\n            labels.append(generate_label(i))\n\n        print(\"Boxes :\" , boxes)\n        print(\"Labels : \", labels)\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n\n        print(\"target : \", target)\n        return target\n\ndef plot_image(img_path, annotation):\n    \n    img = mpimg.imread(img_path)\n    print(img.shape)\n    fig,ax = plt.subplots(1)\n    ax.imshow(img)\n\n    \n    for idx in range(len(annotation[\"boxes\"])):\n        xmin, ymin, xmax, ymax = annotation[\"boxes\"][idx]\n\n        if annotation['labels'][idx] == 0 :\n            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='r',facecolor='none')\n        \n        elif annotation['labels'][idx] == 1 :            \n            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='g',facecolor='none')\n            \n        else :        \n            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='orange',facecolor='none')\n\n        ax.add_patch(rect)\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.188260Z","iopub.execute_input":"2023-08-16T15:12:04.189055Z","iopub.status.idle":"2023-08-16T15:12:04.210186Z","shell.execute_reply.started":"2023-08-16T15:12:04.189023Z","shell.execute_reply":"2023-08-16T15:12:04.209136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imgID = img_list.index('TBX11K/imgs/tb/tb0096.png')\nbbox = generate_target(annot_list[imgID])\nplot_image(img_list[imgID], bbox)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.215599Z","iopub.execute_input":"2023-08-16T15:12:04.215864Z","iopub.status.idle":"2023-08-16T15:12:04.740095Z","shell.execute_reply.started":"2023-08-16T15:12:04.215840Z","shell.execute_reply":"2023-08-16T15:12:04.733282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  3 - Albumentations ","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade albumentations","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.740788Z","iopub.status.idle":"2023-08-16T15:12:04.741143Z","shell.execute_reply.started":"2023-08-16T15:12:04.740963Z","shell.execute_reply":"2023-08-16T15:12:04.740980Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport glob\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom bs4 import BeautifulSoup\n\ndef generate_box(obj, width, height):\n    width_ratio = 512 / float(width)\n    height_ratio = 512 / float(height)\n    \n    xmin = float(obj.find('xmin').text)\n    ymin = float(obj.find('ymin').text)\n    xmax = float(obj.find('xmax').text)\n    ymax = float(obj.find('ymax').text)\n    \n    true_xmin = xmin * width_ratio\n    true_ymin = ymin * height_ratio\n    true_xmax = true_xmin + ((xmax - xmin) * width_ratio)\n    true_ymax = true_ymin + ((ymax - ymin) * height_ratio)\n\n    return [true_xmin, true_ymin, true_xmax, true_ymax]\n\n\ndef generate_label(obj):\n    if obj.find('name').text == \"ActiveTuberculosis\":\n        return 1\n    elif obj.find('name').text == \"ObsoletePulmonaryTuberculosis\":\n        return 2\n    return 0\n\ndef generate_target(file): \n    with open(file) as f:\n        data = f.read()\n        soup = BeautifulSoup(data, \"html.parser\")\n        width_element = soup.find('width')\n        if width_element:\n            width = width_element.get_text()\n        else:\n            width = 1811\n\n        height_element = soup.find('height')\n        if height_element:\n            height = height_element.get_text()\n        else:\n            height = 2022\n            \n        objects = soup.find_all(\"object\")\n        num_objs = len(objects)\n\n        boxes = []\n        labels = []\n        for i in objects:\n            boxes.append(generate_box(i, width, height))\n            labels.append(generate_label(i))\n\n        boxes = torch.as_tensor(boxes, dtype=torch.float32) \n        labels = torch.as_tensor(labels, dtype=torch.int64) \n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        \n        #print(\"target : \", target)\n        \n        return target\n\ndef plot_image_from_output(img, annotation):\n    \n    img = img.permute(1,2,0)\n    \n    fig,ax = plt.subplots(1)\n    ax.imshow(img)\n    \n    for idx in range(len(annotation[\"boxes\"])):\n        xmin, ymin, xmax, ymax = annotation[\"boxes\"][idx]\n\n        if annotation['labels'][idx] == 0 :\n            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='r',facecolor='none')\n        \n        elif annotation['labels'][idx] == 1 :\n            \n            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='g',facecolor='none')\n            \n        else :\n        \n            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='orange',facecolor='none')\n\n        ax.add_patch(rect)\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.743136Z","iopub.status.idle":"2023-08-16T15:12:04.743698Z","shell.execute_reply.started":"2023-08-16T15:12:04.743445Z","shell.execute_reply":"2023-08-16T15:12:04.743474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nimport cv2\nimport numpy as np\nimport time\nimport torch\nimport torchvision\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nimport albumentations\nimport albumentations.pytorch\nfrom matplotlib import pyplot as plt\nimport os\nimport random\n\nclass TorchvisionMaskDataset(Dataset):\n    def __init__(self, path, transform=None):\n        self.path = path\n        self.imgs = list(sorted(os.listdir(self.path)))\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.imgs)\n\n    def __getitem__(self, idx):\n        file_image = self.imgs[idx]\n        file_label = self.imgs[idx][:-3] + 'xml'\n        img_path = os.path.join(self.path, file_image)\n        \n        if 'test' in self.path:\n            label_path = os.path.join(\"test_annotations/\", file_label)\n        else:\n            label_path = os.path.join(\"/kaggle/working/TBX11K/annotations/xml/\", file_label)\n\n        img = Image.open(img_path).convert(\"RGB\")\n        \n        target = generate_target(label_path)\n        start_t = time.time()\n        if self.transform:\n            img = self.transform(img)\n        total_time = (time.time() - start_t)\n\n        return img, target, total_time","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.745427Z","iopub.status.idle":"2023-08-16T15:12:04.745967Z","shell.execute_reply.started":"2023-08-16T15:12:04.745713Z","shell.execute_reply":"2023-08-16T15:12:04.745736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torchvision_transform = transforms.Compose([\n    transforms.Resize((512, 512)), \n    #transforms.RandomCrop(224),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n    transforms.RandomHorizontalFlip(p = 1),\n    transforms.ToTensor(),\n])\n\ntorchvision_dataset = TorchvisionMaskDataset(\n    path = '/kaggle/working/TBX11K/imgs/tb/',\n    transform = torchvision_transform\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.747604Z","iopub.status.idle":"2023-08-16T15:12:04.748212Z","shell.execute_reply.started":"2023-08-16T15:12:04.747955Z","shell.execute_reply":"2023-08-16T15:12:04.747978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"only_totensor = transforms.Compose([transforms.ToTensor()])\n\ntorchvision_dataset_no_transform = TorchvisionMaskDataset(\n    path = '/kaggle/working/TBX11K/imgs/tb/',\n    transform = only_totensor\n)\n\nimg, annot, transform_time = torchvision_dataset_no_transform[0]\nprint('Before applying transforms')\nplot_image_from_output(img, annot)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.749894Z","iopub.status.idle":"2023-08-16T15:12:04.750309Z","shell.execute_reply.started":"2023-08-16T15:12:04.750066Z","shell.execute_reply":"2023-08-16T15:12:04.750083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img, annot, transform_time = torchvision_dataset[0]\n\nprint('After applying transforms')\nplot_image_from_output(img, annot)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.751914Z","iopub.status.idle":"2023-08-16T15:12:04.752685Z","shell.execute_reply.started":"2023-08-16T15:12:04.752439Z","shell.execute_reply":"2023-08-16T15:12:04.752462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_time = 0\nfor i in range(100):\n  sample, _, transform_time = torchvision_dataset[0]\n  total_time += transform_time\n\nprint(\"torchvision time: {} ms\".format(total_time*10))","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.754177Z","iopub.status.idle":"2023-08-16T15:12:04.755410Z","shell.execute_reply.started":"2023-08-16T15:12:04.755159Z","shell.execute_reply":"2023-08-16T15:12:04.755184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### 3.1.2 Albumentations","metadata":{}},{"cell_type":"code","source":"class AlbumentationsDataset(Dataset):\n    def __init__(self, path, transform=None):\n        self.path = path\n        self.imgs = list(sorted(os.listdir(self.path)))\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.imgs)\n\n    def __getitem__(self, idx):\n        file_image = self.imgs[idx]\n        file_label = self.imgs[idx][:-3] + 'xml'\n        img_path = os.path.join(self.path, file_image)\n        print(img_path)\n\n        if 'test' in self.path:\n            label_path = os.path.join(\"test_annotations/\", file_label)\n        else:\n            label_path = os.path.join(\"/kaggle/working/TBX11K/annotations/xml/\", file_label)\n        \n        \n        # Read an image with OpenCV\n        image = cv2.imread(img_path)\n        if image is None or image.size == 0:\n            print(\"Image is empty or could not be loaded.\")\n        else:\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            target = generate_target(label_path)\n\n            start_t = time.time()\n            if self.transform:\n                augmented = self.transform(image=image)\n                total_time = (time.time() - start_t)\n                image = augmented['image']\n                #print(\"Inside AlbumentationsDataset Image after Transform: \", image)   \n\n        return image, target, total_time","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.756776Z","iopub.status.idle":"2023-08-16T15:12:04.757259Z","shell.execute_reply.started":"2023-08-16T15:12:04.757004Z","shell.execute_reply":"2023-08-16T15:12:04.757027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Same transform with torchvision_transform\nalbumentations_transform = albumentations.Compose([\n    albumentations.Resize(300, 300), \n    #albumentations.RandomCrop(224, 224),\n    albumentations.ColorJitter(p=1), \n    albumentations.HorizontalFlip(p=1), \n    albumentations.pytorch.transforms.ToTensorV2()\n])","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.758927Z","iopub.status.idle":"2023-08-16T15:12:04.759399Z","shell.execute_reply.started":"2023-08-16T15:12:04.759163Z","shell.execute_reply":"2023-08-16T15:12:04.759186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Before applying transforms\nimg, annot, transform_time = torchvision_dataset_no_transform[0]\nprint(img)\nplot_image_from_output(img, annot)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.760967Z","iopub.status.idle":"2023-08-16T15:12:04.761469Z","shell.execute_reply.started":"2023-08-16T15:12:04.761223Z","shell.execute_reply":"2023-08-16T15:12:04.761246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# After applying transforms\nalbumentation_dataset = AlbumentationsDataset(\n    path = '/kaggle/working/TBX11K/imgs/tb/',\n    transform = albumentations_transform\n)\n\nimg, annot, transform_time = albumentation_dataset[0]\nplot_image_from_output(img, annot)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.763823Z","iopub.status.idle":"2023-08-16T15:12:04.764527Z","shell.execute_reply.started":"2023-08-16T15:12:04.764274Z","shell.execute_reply":"2023-08-16T15:12:04.764297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_time = 0\nfor i in range(5):\n    sample, _, transform_time = albumentation_dataset[0]\n    total_time += transform_time\n\nprint(\"albumentations time/sample: {} ms\".format(total_time*10))","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.765941Z","iopub.status.idle":"2023-08-16T15:12:04.766771Z","shell.execute_reply.started":"2023-08-16T15:12:04.766529Z","shell.execute_reply":"2023-08-16T15:12:04.766553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.1.3 Probability based Augmentations ","metadata":{}},{"cell_type":"code","source":"albumentations_transform_oneof = albumentations.Compose([\n    albumentations.Resize(300, 300), \n    #albumentations.RandomCrop(224, 224),\n    albumentations.OneOf([\n                          albumentations.HorizontalFlip(p=1),\n                          albumentations.RandomRotate90(p=1),\n                          albumentations.VerticalFlip(p=1)            \n    ], p=1),\n    albumentations.OneOf([\n                          albumentations.MotionBlur(p=1),\n                          albumentations.OpticalDistortion(p=1),\n                          albumentations.GaussNoise(p=1)                 \n    ], p=1),\n    albumentations.pytorch.ToTensorV2()\n])","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.768333Z","iopub.status.idle":"2023-08-16T15:12:04.769396Z","shell.execute_reply.started":"2023-08-16T15:12:04.769145Z","shell.execute_reply":"2023-08-16T15:12:04.769170Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"albumentation_dataset_oneof = AlbumentationsDataset(\n    path = '/kaggle/working/TBX11K/imgs/tb/',\n    transform = albumentations_transform_oneof\n)\n\nnum_samples = 10\nfig, ax = plt.subplots(1, num_samples, figsize=(25, 5))\nfor i in range(num_samples):\n  ax[i].imshow(transforms.ToPILImage()(albumentation_dataset_oneof[0][0]))\n  ax[i].axis('off')\n","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.770678Z","iopub.status.idle":"2023-08-16T15:12:04.771490Z","shell.execute_reply.started":"2023-08-16T15:12:04.771243Z","shell.execute_reply":"2023-08-16T15:12:04.771267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2. Bounding Box Augmentation","metadata":{}},{"cell_type":"code","source":"import cv2","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.773019Z","iopub.status.idle":"2023-08-16T15:12:04.773934Z","shell.execute_reply.started":"2023-08-16T15:12:04.773649Z","shell.execute_reply":"2023-08-16T15:12:04.773675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BboxAugmentationDataset(Dataset):\n    def __init__(self, path, transform=None):\n        self.path = path\n        self.imgs = list(sorted(os.listdir(self.path)))\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.imgs)\n\n    def __getitem__(self, idx):\n        file_image = self.imgs[idx]\n        file_label = self.imgs[idx][:-3] + 'xml'\n        img_path = os.path.join(self.path, file_image)\n\n        if 'test' in self.path:\n            label_path = os.path.join(\"test_annotations/\", file_label)\n        else:\n            label_path = os.path.join(\"/kaggle/working/TBX11K/annotations/xml/\", file_label)\n        \n        image = cv2.imread(img_path)\n        \n        if image is None:\n            raise ValueError(f\"Image not found or could not be loaded: {img_path}\")\n        \n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        target = generate_target(label_path)\n\n        if self.transform:\n            transformed = self.transform(image = image, bboxes = target['boxes'], labels = target['labels'])\n            image = transformed['image']\n            target = {'boxes':transformed['bboxes'], 'labels':transformed['labels']}\n        \n            \n        return image, target","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.775838Z","iopub.status.idle":"2023-08-16T15:12:04.776834Z","shell.execute_reply.started":"2023-08-16T15:12:04.776563Z","shell.execute_reply":"2023-08-16T15:12:04.776586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bbox_transform = albumentations.Compose(\n    [albumentations.HorizontalFlip(p=1),\n     albumentations.Rotate(p=1),\n     albumentations.pytorch.transforms.ToTensorV2()],\n    bbox_params=albumentations.BboxParams(format='pascal_voc', label_fields=['labels']),\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.778127Z","iopub.status.idle":"2023-08-16T15:12:04.779180Z","shell.execute_reply.started":"2023-08-16T15:12:04.778919Z","shell.execute_reply":"2023-08-16T15:12:04.778943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bbox_transform_dataset = BboxAugmentationDataset(\n    path = '/kaggle/working/TBX11K/imgs/tb/',\n    transform = bbox_transform\n)\n\nimg, annot = bbox_transform_dataset[0]\nplot_image_from_output(img, annot)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.780453Z","iopub.status.idle":"2023-08-16T15:12:04.781249Z","shell.execute_reply.started":"2023-08-16T15:12:04.780991Z","shell.execute_reply":"2023-08-16T15:12:04.781014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.3 Data Seperation","metadata":{}},{"cell_type":"code","source":"print(len(os.listdir('/kaggle/working/TBX11K/annotations/xml/')))\nprint(len(os.listdir('/kaggle/working/TBX11K/imgs/tb/')))","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.782854Z","iopub.status.idle":"2023-08-16T15:12:04.783395Z","shell.execute_reply.started":"2023-08-16T15:12:04.783147Z","shell.execute_reply":"2023-08-16T15:12:04.783172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/\n\n!mkdir test_images\n!mkdir test_annotations","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.785454Z","iopub.status.idle":"2023-08-16T15:12:04.785906Z","shell.execute_reply.started":"2023-08-16T15:12:04.785674Z","shell.execute_reply":"2023-08-16T15:12:04.785698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nrandom.seed(1234)\nidx = random.sample(range(800), 300)\nprint(len(idx))\nprint(idx[:10])","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.787851Z","iopub.status.idle":"2023-08-16T15:12:04.788865Z","shell.execute_reply.started":"2023-08-16T15:12:04.788617Z","shell.execute_reply":"2023-08-16T15:12:04.788641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(np.array(sorted(os.listdir('/kaggle/working/TBX11K/imgs/tb/'))))","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.789954Z","iopub.status.idle":"2023-08-16T15:12:04.790743Z","shell.execute_reply.started":"2023-08-16T15:12:04.790503Z","shell.execute_reply":"2023-08-16T15:12:04.790527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pwd","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.792040Z","iopub.status.idle":"2023-08-16T15:12:04.792848Z","shell.execute_reply.started":"2023-08-16T15:12:04.792592Z","shell.execute_reply":"2023-08-16T15:12:04.792615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(np.array(sorted(os.listdir('/kaggle/working/TBX11K/imgs/tb/'))))","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.794326Z","iopub.status.idle":"2023-08-16T15:12:04.795185Z","shell.execute_reply.started":"2023-08-16T15:12:04.794923Z","shell.execute_reply":"2023-08-16T15:12:04.794947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport shutil\n\nfor img in np.array(sorted(os.listdir('/kaggle/working/TBX11K/imgs/tb/')))[idx]:\n    shutil.move('/kaggle/working/TBX11K/imgs/tb/'+ img, '/kaggle/working/test_images/'+img)\n\nfor annot in np.array(sorted(os.listdir('/kaggle/working/TBX11K/annotations/xml/')))[idx]:\n    shutil.move('/kaggle/working/TBX11K/annotations/xml/'+ annot, '/kaggle/working/test_annotations/'+ annot)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.796530Z","iopub.status.idle":"2023-08-16T15:12:04.797341Z","shell.execute_reply.started":"2023-08-16T15:12:04.797082Z","shell.execute_reply":"2023-08-16T15:12:04.797120Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(os.listdir('/kaggle/working/TBX11K/annotations/xml/')))\nprint(len(os.listdir('/kaggle/working/TBX11K/imgs/tb/')))\nprint(len(os.listdir('test_annotations')))\nprint(len(os.listdir('test_images')))","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.798682Z","iopub.status.idle":"2023-08-16T15:12:04.799665Z","shell.execute_reply.started":"2023-08-16T15:12:04.799411Z","shell.execute_reply":"2023-08-16T15:12:04.799441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(os.listdir('/kaggle/working/TBX11K/imgs/tb/')))","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.800890Z","iopub.status.idle":"2023-08-16T15:12:04.801844Z","shell.execute_reply.started":"2023-08-16T15:12:04.801598Z","shell.execute_reply":"2023-08-16T15:12:04.801622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport pandas as pd\nfrom collections import Counter\n\ndef get_num_objects_for_each_class(dataset):\n\n    total_labels = []\n\n    for img, annot in tqdm(dataset, position = 0, leave = True):\n        total_labels += [int(i) for i in annot['labels']]\n\n    return Counter(total_labels)\n\n\ntrain_data =  BboxAugmentationDataset(\n    path = '/kaggle/working/TBX11K/imgs/tb/'\n)\n\ntest_data =  BboxAugmentationDataset(\n    path = '/kaggle/working/test_images/'\n)\n\ntrain_objects = get_num_objects_for_each_class(train_data)\ntest_objects = get_num_objects_for_each_class(test_data)\n\nprint('\\n Object in training data', train_objects)\nprint('\\n Object in test data', test_objects)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.802993Z","iopub.status.idle":"2023-08-16T15:12:04.803351Z","shell.execute_reply.started":"2023-08-16T15:12:04.803179Z","shell.execute_reply":"2023-08-16T15:12:04.803196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#np.array(sorted(os.listdir('images')))","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.805982Z","iopub.status.idle":"2023-08-16T15:12:04.806758Z","shell.execute_reply.started":"2023-08-16T15:12:04.806512Z","shell.execute_reply":"2023-08-16T15:12:04.806536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport shutil\n\nprint(len(os.listdir('/kaggle/working/TBX11K/annotations')))\nprint(len(os.listdir('/kaggle/working/TBX11K/imgs')))\n\n#!mkdir test_images\n#!mkdir test_annotations\n\n\nrandom.seed(1234)\nidx = random.sample(range(853), 170)\n\n#for img in np.array(sorted(os.listdir('images')))[idx]:\n    #shutil.move('images/'+img, 'test_images/'+img)\n\n#for annot in np.array(sorted(os.listdir('/kaggle/working/test_annotations')))[idx]:\n    #shutil.move('/kaggle/working/TBX11K/annotations/'+annot, '/kaggle/working/test_annotations/'+annot)\n\nprint(len(os.listdir('/kaggle/working/TBX11K/annotations')))\nprint(len(os.listdir('/kaggle/working/TBX11K/imgs')))\nprint(len(os.listdir('/kaggle/working/test_annotations')))\nprint(len(os.listdir('/kaggle/working/test_images')))","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.808177Z","iopub.status.idle":"2023-08-16T15:12:04.808949Z","shell.execute_reply.started":"2023-08-16T15:12:04.808688Z","shell.execute_reply":"2023-08-16T15:12:04.808711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(os.listdir('/kaggle/working/TBX11K/annotations')))\nprint(len(os.listdir('/kaggle/working/TBX11K/imgs')))\nprint(len(os.listdir('/kaggle/working/test_annotations')))\nprint(len(os.listdir('/kaggle/working/test_images')))","metadata":{"execution":{"iopub.status.busy":"2023-08-16T20:05:06.777938Z","iopub.execute_input":"2023-08-16T20:05:06.778548Z","iopub.status.idle":"2023-08-16T20:05:07.300032Z","shell.execute_reply.started":"2023-08-16T20:05:06.778509Z","shell.execute_reply":"2023-08-16T20:05:07.298588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(os.listdir('/kaggle/working/TBX11K/annotations')))\nprint(len(os.listdir('/kaggle/working/TBX11K/imgs')))\nprint(len(os.listdir('/kaggle/working/test_annotations')))\nprint(len(os.listdir('/kaggle/working/test_images')))","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.812521Z","iopub.status.idle":"2023-08-16T15:12:04.813337Z","shell.execute_reply.started":"2023-08-16T15:12:04.813057Z","shell.execute_reply":"2023-08-16T15:12:04.813082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.3 Defining Dataset Class","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nimport cv2\nimport numpy as np\nimport time\nimport torch\nimport torchvision\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nimport albumentations\nimport albumentations.pytorch\nfrom matplotlib import pyplot as plt\nimport os\nimport random","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:51:00.778526Z","iopub.execute_input":"2023-08-16T15:51:00.778919Z","iopub.status.idle":"2023-08-16T15:51:00.785369Z","shell.execute_reply.started":"2023-08-16T15:51:00.778886Z","shell.execute_reply":"2023-08-16T15:51:00.784034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Same transform with torchvision_transform\nalbumentations_transform_TRAIN = albumentations.Compose([\n    albumentations.Resize(512, 512), \n    albumentations.RandomCrop(500, 500),###\n    albumentations.ColorJitter(p=1.0), \n    albumentations.HorizontalFlip(p=1), ##\n    albumentations.RandomBrightnessContrast(p=0.2),     \n    albumentations.RandomFog(p = 1),         ##\n  #A.VerticalFlip(p = 1),\n  #albumentations.RandomBrightness(p = 1),\n  #albumentations.RandomContrast(limit = 0.6, p = 1),\n  #albumentations.RandomCrop(p = 1,height = 450, width = 450),\n  #albumentations.Rotate(p = 1, limit = 90),\n    albumentations.RGBShift(p = 1.0),\n    albumentations.HueSaturationValue(p=0.35),\n    albumentations.Normalize(mean=(0, 0, 0), std=(1, 1, 1), max_pixel_value=255.0, p=1.0),\n    albumentations.ToGray(p = 1),\n    albumentations.pytorch.transforms.ToTensorV2()],    \n    bbox_params=albumentations.BboxParams(format='pascal_voc', label_fields=['labels'])\n    )\n\n\n# Same transform with torchvision_transform\nalbumentations_transform_TEST = albumentations.Compose([\n    albumentations.Resize(512, 512),   \n    albumentations.Normalize(mean=(0, 0, 0), std=(1, 1, 1), max_pixel_value=255.0, p=1.0),\n    albumentations.pytorch.transforms.ToTensorV2()],\n    bbox_params=albumentations.BboxParams(format='pascal_voc', label_fields=['labels'])\n    )\n\n\n#bbox_transform = albumentations.Compose(\n    #[albumentations.HorizontalFlip(p=1),\n    #albumentations.Rotate(p=1),\n     #albumentations.pytorch.transforms.ToTensorV2()],\n   # bbox_params=albumentations.BboxParams(format='pascal_voc', label_fields=['labels']),\n#)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T19:27:49.762697Z","iopub.execute_input":"2023-08-16T19:27:49.763476Z","iopub.status.idle":"2023-08-16T19:27:49.774423Z","shell.execute_reply.started":"2023-08-16T19:27:49.763435Z","shell.execute_reply":"2023-08-16T19:27:49.773155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Image Value DistributionÂ¶\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.819529Z","iopub.status.idle":"2023-08-16T15:12:04.820490Z","shell.execute_reply.started":"2023-08-16T15:12:04.820207Z","shell.execute_reply":"2023-08-16T15:12:04.820235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_val = []\nstd_dev_val = []\nmax_val = []\nmin_val = []\n\nfor i in range(0, samples):\n    mean_val.append(data['image'][i].mean())\n    std_dev_val.append(np.std(data['image'][i]))\n    max_val.append(data['image'][i].max())\n    min_val.append(data['image'][i].min())\n\nimageEDA = data.loc[:,['image','corona_result','path']]\nimageEDA['mean'] = mean_val\nimageEDA['stedev'] = std_dev_val\nimageEDA['max'] = max_val\nimageEDA['min'] = min_val\n\nimageEDA['subt_mean'] = imageEDA['mean'].mean() - imageEDA['mean']","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.822158Z","iopub.status.idle":"2023-08-16T15:12:04.822967Z","shell.execute_reply.started":"2023-08-16T15:12:04.822708Z","shell.execute_reply":"2023-08-16T15:12:04.822731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##from : https://www.kaggle.com/code/danushkumarv/covid-19-cnn-grad-cam-viz#5-%7C-Conclusion\n\n##and from https://www.kaggle.com/code/sana306/detection-of-covid-positive-cases-using-dl#Saving-Model\n\nimport albumentations as A\nchosen_image = cv2.imread(\"/kaggle/input/tbx-11/TBX11K/imgs/tb/tb0005.png\")\n\nalbumentation_list = [A.RandomFog(p = 1),\n                      A.VerticalFlip(p = 1),\n                      A.RandomBrightness(p = 1),\n                      A.RandomContrast(limit = 0.6, p = 1),\n                      A.RandomCrop(p = 1,height = 450, width = 450),\n                      A.Rotate(p = 1, limit = 90),\n                      A.RGBShift(p = 1),\n                      A.HueSaturationValue(p=0.35),\n                      A.Normalize(mean=(0, 0, 0), std=(1, 1, 1), max_pixel_value=255.0, p=1.0),\n                      A.ToGray(p = 1)]\n\nimg_matrix_list = []\nbboxes_list = []\nfor aug_type in albumentation_list:\n    img = aug_type(image = chosen_image)['image']\n    img_matrix_list.append(img)\n\nimg_matrix_list.insert(0,chosen_image)    \n\ntitles_list = [\"Original\", \"VerticalFlip\", \"RandomFog\",\n               \"RandomContrast\", \"RandomBrightness\", \n               \"RandomCrop\", \"Rotate\", \"RGBShift\",\"HueSaturationValue\",\"Gray\"]\n\nplot_multiple_img(img_matrix_list, titles_list, ncols = 5, main_title = \"Different Types of Augmentations\")","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.824448Z","iopub.status.idle":"2023-08-16T15:12:04.825511Z","shell.execute_reply.started":"2023-08-16T15:12:04.825053Z","shell.execute_reply":"2023-08-16T15:12:04.825081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_multiple_img(img_matrix_list, title_list, ncols, main_title = \"\"):\n    \n    fig, myaxes = plt.subplots(figsize = (15, 8), nrows = 2, ncols = ncols, squeeze = False)\n    fig.suptitle(main_title, fontsize = 18)\n    fig.subplots_adjust(wspace = 0.3)\n    fig.subplots_adjust(hspace = 0.3)\n    \n    for i, (img, title) in enumerate(zip(img_matrix_list, title_list)):\n        myaxes[i // ncols][i % ncols].imshow(img)\n        myaxes[i // ncols][i % ncols].set_title(title, fontsize = 15)\n        \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.827580Z","iopub.status.idle":"2023-08-16T15:12:04.828463Z","shell.execute_reply.started":"2023-08-16T15:12:04.828212Z","shell.execute_reply":"2023-08-16T15:12:04.828238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport glob\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport matplotlib.patches as patches\nfrom bs4 import BeautifulSoup\nfrom PIL import Image\nimport cv2\nimport numpy as np\nimport time\nimport torch\nimport torchvision\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nfrom matplotlib import pyplot as plt\nimport os\n\ndef generate_box(obj, width, height):\n    width_ratio = 512 / float(width)\n    height_ratio = 512 / float(height)\n    \n    xmin = float(obj.find('xmin').text)\n    ymin = float(obj.find('ymin').text)\n    xmax = float(obj.find('xmax').text)\n    ymax = float(obj.find('ymax').text)\n    \n    true_xmin = xmin * width_ratio\n    true_ymin = ymin * height_ratio\n    true_xmax = true_xmin + ((xmax - xmin) * width_ratio)\n    true_ymax = true_ymin + ((ymax - ymin) * height_ratio)\n\n    return [true_xmin, true_ymin, true_xmax, true_ymax]\n\ndef generate_label(obj):\n    if obj.find('name').text == \"ActiveTuberculosis\":\n        return 0\n    elif obj.find('name').text == \"ObsoletePulmonaryTuberculosis\":\n        return 1\n    return 2\n\ndef generate_target(file): \n    with open(file) as f:\n        data = f.read()\n        soup = BeautifulSoup(data, \"html.parser\")\n        \n        width_element = soup.find('width')\n        if width_element:\n            width = width_element.get_text()\n        else:\n            print(\" i am here\")\n            width = 1811\n\n        height_element = soup.find('height')\n        if height_element:\n            height = height_element.get_text()\n        else:\n            print(\" i am here\")\n            height = 2022\n            \n        objects = soup.find_all(\"object\")\n\n        num_objs = len(objects)\n        #print(\"File name in generate_target : \",file )\n        boxes = []\n        labels = []\n        for i in objects:            \n            boxes.append(generate_box(i, width, height))\n            labels.append(generate_label(i))\n              \n        boxes = torch.as_tensor(boxes, dtype=torch.float32) \n        labels = torch.as_tensor(labels, dtype=torch.int64) \n    \n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n                \n        \n        return target\n\ndef plot_image_from_output(img, annotation):\n    \n    img = img.cpu().permute(1,2,0)\n    \n    rects = []\n\n    for idx in range(len(annotation[\"boxes\"])):\n        xmin, ymin, xmax, ymax = annotation[\"boxes\"][idx]\n\n        if annotation['labels'][idx] == 0 :\n            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='r',facecolor='none')\n        \n        elif annotation['labels'][idx] == 1 :\n            \n            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='g',facecolor='none')\n            \n        else :\n        \n            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='orange',facecolor='none')\n\n        rects.append(rect)\n\n    return img, rects\n\nclass MaskDataset(Dataset):\n    def __init__(self, path, transform=None):\n        self.path = path\n        self.imgs = list(sorted(os.listdir(self.path)))\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.imgs)\n\n    def __getitem__(self, idx):\n        file_image = self.imgs[idx]\n        file_label = self.imgs[idx][:-3] + 'xml'\n        img_path = os.path.join(self.path, file_image)\n        \n        if 'test' in self.path:\n            label_path = os.path.join(\"/kaggle/working/test_annotations/\", file_label)\n        else:\n            label_path = os.path.join(\"/kaggle/working/TBX11K/annotations/xml/\", file_label)\n            #print(\"Label path : \",label_path)\n\n        img = Image.open(img_path).convert(\"RGB\")\n        \n        #print(\"Image in Mask: \",img )\n        #print(\"Img path in Mask: \",img_path )\n        #print(\"File_Label in MaskDataset : \" , label_path)\n        \n        #print(\"Image in MaskDataset : \", img_path)\n        target = generate_target(label_path)\n        #print(\"Image in Maskdataset :\", img_path )\n        #print(\"Target in Maskdataset:\", target )\n        boxes = np.array(target['boxes'])\n        labels = np.array(target['labels'])              \n        #print(\"Target in MaskDataset : \", target)\n        to_tensor = torchvision.transforms.ToTensor()\n\n        if self.transform:\n            #print(\"IN TRANSFORM :\" ,self.transform)\n            #img, transform_target = self.transform(image = np.array(img), boxes = np.array(target['boxes']))\n            augmented = self.transform(image = np.array(img), bboxes = boxes , labels = labels)\n            #print(\"hi.. this is rachana\")\n            img_augmented = augmented['image']\n            box_augmented = augmented['bboxes']\n            lbl_augmented = augmented['labels']  \n            img = img_augmented\n            #if (img_path == '/kaggle/working/TBX11K/imgs/tb/tb0206.png'):\n                         \n            #target = {'boxes':augmented['bboxes'], 'labels':augmented['labels']}\n            target['boxes'] = torch.as_tensor(box_augmented, dtype=torch.float32)   # 16/8                            \n            target['labels'] = torch.as_tensor(lbl_augmented, dtype=torch.int64) # 16/8\n            \n            #print(\" Inside Transform :\",img )\n            #print(\" Inside Transform :\",target['boxes'] )\n            #print(\" Inside Transform :\",target['labels'] )\n            \n            #target = {'boxes':torch.as_tensor(augmented['bboxes'],dtype=torch.float32), 'labels': torch.as_tensor(augmented['labels'],dtype=torch.int64)}\n           \n            \n        # change to tensor\n        #img = to_tensor(img) \n        #print(\"Image shape : \", img.shape)\n        \n        return img, target\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\ndataset = MaskDataset('/kaggle/working/TBX11K/imgs/tb/', transform=albumentations_transform_TRAIN)\ntest_dataset =MaskDataset('/kaggle/working/test_images/',transform=albumentations_transform_TEST)\n\n\ndata_loader = torch.utils.data.DataLoader(dataset, batch_size=1, collate_fn=collate_fn)\ntest_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, collate_fn=collate_fn)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T19:27:56.547538Z","iopub.execute_input":"2023-08-16T19:27:56.547902Z","iopub.status.idle":"2023-08-16T19:27:56.579151Z","shell.execute_reply.started":"2023-08-16T19:27:56.547871Z","shell.execute_reply":"2023-08-16T19:27:56.578092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(dataset)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.832506Z","iopub.status.idle":"2023-08-16T15:12:04.833377Z","shell.execute_reply.started":"2023-08-16T15:12:04.833040Z","shell.execute_reply":"2023-08-16T15:12:04.833067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(data_loader)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.835001Z","iopub.status.idle":"2023-08-16T15:12:04.835781Z","shell.execute_reply.started":"2023-08-16T15:12:04.835536Z","shell.execute_reply":"2023-08-16T15:12:04.835560Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.4 Import Model","metadata":{}},{"cell_type":"code","source":"torch==1.7.0+cu101 torchvision==0.8.1+cu101 torchaudio==0.7.0","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.837968Z","iopub.status.idle":"2023-08-16T15:12:04.838852Z","shell.execute_reply.started":"2023-08-16T15:12:04.838563Z","shell.execute_reply":"2023-08-16T15:12:04.838591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchvision\n\nimport torchaudio","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.840421Z","iopub.status.idle":"2023-08-16T15:12:04.841276Z","shell.execute_reply.started":"2023-08-16T15:12:04.841010Z","shell.execute_reply":"2023-08-16T15:12:04.841035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torchvision.__version__","metadata":{"execution":{"iopub.status.busy":"2023-08-16T16:13:57.193189Z","iopub.execute_input":"2023-08-16T16:13:57.194211Z","iopub.status.idle":"2023-08-16T16:13:57.204032Z","shell.execute_reply.started":"2023-08-16T16:13:57.194174Z","shell.execute_reply":"2023-08-16T16:13:57.203076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torch==1.7.0+cu101 torchvision==0.8.1+cu101 torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html","metadata":{"execution":{"iopub.status.busy":"2023-08-16T16:14:22.215222Z","iopub.execute_input":"2023-08-16T16:14:22.215707Z","iopub.status.idle":"2023-08-16T16:14:28.485887Z","shell.execute_reply.started":"2023-08-16T16:14:22.215674Z","shell.execute_reply":"2023-08-16T16:14:28.484635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"retina = torchvision.models.detection.retinanet_resnet50_fpn(num_classes = 2, pretrained=False, pretrained_backbone = True)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T17:03:21.062089Z","iopub.execute_input":"2023-08-16T17:03:21.063248Z","iopub.status.idle":"2023-08-16T17:03:21.642419Z","shell.execute_reply.started":"2023-08-16T17:03:21.063200Z","shell.execute_reply":"2023-08-16T17:03:21.641375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.5 Transfer Learning","metadata":{}},{"cell_type":"code","source":"\"\"\"\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\nnum_epochs = 2\nretina.to(device)\n    \n# parameters\nparams = [p for p in retina.parameters() if p.requires_grad] # select parameters that require gradient calculation\noptimizer = torch.optim.SGD(params, lr=0.005,\n                                momentum=0.9, weight_decay=0.0005)\n\nlen_dataloader = len(data_loader)\nto_tensor = torchvision.transforms.ToTensor()\n\n# about 4 min per epoch on Colab GPU\nfor epoch in range(num_epochs):\n    start = time.time()\n    retina.train()\n\n    i = 0    \n    epoch_loss = 0\n    for images, targets, img_path in data_loader:\n        #print(\"Images in DL : \",img_path)\n             \n        images = list(image.to(device) for image in images)\n        \n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        #print(\"Images : \", images)\n        #print(\"Targets : \", targets)\n        loss_dict = retina(images, targets) \n\n        losses = sum(loss for loss in loss_dict.values()) \n\n        i += 1\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n        \n        epoch_loss += losses \n    print(epoch_loss, f'time: {time.time() - start}')","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.849329Z","iopub.status.idle":"2023-08-16T15:12:04.850094Z","shell.execute_reply.started":"2023-08-16T15:12:04.849851Z","shell.execute_reply":"2023-08-16T15:12:04.849874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd working","metadata":{"execution":{"iopub.status.busy":"2023-08-16T19:07:07.029925Z","iopub.execute_input":"2023-08-16T19:07:07.030453Z","iopub.status.idle":"2023-08-16T19:07:07.040244Z","shell.execute_reply.started":"2023-08-16T19:07:07.030410Z","shell.execute_reply":"2023-08-16T19:07:07.038127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport time\n#from torchvision.models.detection.retinanet import RetinaNetLoss\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\nnum_epochs = 5\nretina.to(device)\n    \n# parameters\nparams = [p for p in retina.parameters() if p.requires_grad] # select parameters that require gradient calculation\noptimizer = torch.optim.SGD(params, lr=0.005,\n                                momentum=0.9, weight_decay=0.0005)\n\n# Loss function\n#criterion = RetinaNetLoss(num_classes=3)\n\nlen_dataloader = len(data_loader)\n\n# about 4 min per epoch on Colab GPU\nfor epoch in range(num_epochs):\n    start = time.time()\n    retina.train()\n\n    i = 0    \n    epoch_loss = 0\n    for images, targets in data_loader:\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]        \n        #print(\"In Training loop :\", images)\n        #print(\"In Training loop :\", targets)\n        empty_found = False\n        for t in targets:\n            #print(\"All target items : \", t.items)\n            for k, v in t.items():\n                #k = k.to(device)\n                #print(\"K value :\", k)\n                #print(\"V value :\", v)\n                if v.size(0) == 0:\n                    #print(f\"Empty tensor found for the dictionaries.\")\n                    empty_found = True\n\n        if empty_found:\n            print(\" empty tensors found in the 'targets' list of dictionaries.\")\n        else:           \n            \n            \n            # Forward pass for this image and its targets\n            loss_dict = retina(images, targets)\n            \n            #loss_dict = retina(images_batch, targets_batch) \n            \n            #print(\"Loss Dictionary :\", loss_dict)\n\n            losses = sum(loss for loss in loss_dict.values()) \n\n            #print(\"Losses :\", losses)\n            i += 1\n\n            optimizer.zero_grad()\n            losses.backward()\n            \n            ## Print gradients of model parameters\n            #for name, param in retina.named_parameters():\n                #if param.grad is not None:\n                    #print(f\"Gradient for parameter '{name}':\")\n                    #print(param.grad)\n\n                    ## Check for NaN or infinite gradients\n                    #if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():\n                        #print(f\"NaN or Infinite gradients detected for parameter '{name}'!\")\n                #else:\n                    #print(f\"No gradient for parameter '{name}'\")\n            \n            \n            \n            torch.nn.utils.clip_grad_norm_(retina.parameters(), max_norm=1.0)  # Clip gradients\n            optimizer.step()\n\n            epoch_loss += losses \n    print(\"Epoch : \" , epoch)\n    print(epoch_loss, f'time: {time.time() - start}')","metadata":{"execution":{"iopub.status.busy":"2023-08-16T19:28:19.104006Z","iopub.execute_input":"2023-08-16T19:28:19.105164Z","iopub.status.idle":"2023-08-16T19:34:26.732233Z","shell.execute_reply.started":"2023-08-16T19:28:19.105118Z","shell.execute_reply":"2023-08-16T19:34:26.731234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Clip gradients","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.853846Z","iopub.status.idle":"2023-08-16T15:12:04.854801Z","shell.execute_reply.started":"2023-08-16T15:12:04.854495Z","shell.execute_reply":"2023-08-16T15:12:04.854522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pwd","metadata":{"execution":{"iopub.status.busy":"2023-08-16T19:06:52.748811Z","iopub.execute_input":"2023-08-16T19:06:52.749281Z","iopub.status.idle":"2023-08-16T19:06:53.743544Z","shell.execute_reply.started":"2023-08-16T19:06:52.749242Z","shell.execute_reply":"2023-08-16T19:06:53.742172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2023-08-16T19:36:10.454199Z","iopub.execute_input":"2023-08-16T19:36:10.455308Z","iopub.status.idle":"2023-08-16T19:36:10.462719Z","shell.execute_reply.started":"2023-08-16T19:36:10.455258Z","shell.execute_reply":"2023-08-16T19:36:10.461433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(retina.state_dict(),f'retina_{num_epochs}.pt')","metadata":{"execution":{"iopub.status.busy":"2023-08-16T19:36:12.208545Z","iopub.execute_input":"2023-08-16T19:36:12.209578Z","iopub.status.idle":"2023-08-16T19:36:12.538641Z","shell.execute_reply.started":"2023-08-16T19:36:12.209540Z","shell.execute_reply":"2023-08-16T19:36:12.537420Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%cd /kaggle/working/TBX11K/annotations/xml/\n#!ls","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.856214Z","iopub.status.idle":"2023-08-16T15:12:04.856986Z","shell.execute_reply.started":"2023-08-16T15:12:04.856730Z","shell.execute_reply":"2023-08-16T15:12:04.856753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(retina.state_dict(),f'retina_{num_epochs}.pt')\nretina.load_state_dict(torch.load(f'retina_{num_epochs}.pt'))","metadata":{"execution":{"iopub.status.busy":"2023-08-16T19:36:15.651435Z","iopub.execute_input":"2023-08-16T19:36:15.652470Z","iopub.status.idle":"2023-08-16T19:36:16.091435Z","shell.execute_reply.started":"2023-08-16T19:36:15.652433Z","shell.execute_reply":"2023-08-16T19:36:16.090498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nretina.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T19:36:18.280896Z","iopub.execute_input":"2023-08-16T19:36:18.282087Z","iopub.status.idle":"2023-08-16T19:36:18.297547Z","shell.execute_reply.started":"2023-08-16T19:36:18.282039Z","shell.execute_reply":"2023-08-16T19:36:18.296466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(retina.state_dict(),f'retina_{num_epochs}_backup_newalbu.pt')","metadata":{"execution":{"iopub.status.busy":"2023-08-16T19:36:40.150860Z","iopub.execute_input":"2023-08-16T19:36:40.151276Z","iopub.status.idle":"2023-08-16T19:36:40.358158Z","shell.execute_reply.started":"2023-08-16T19:36:40.151242Z","shell.execute_reply":"2023-08-16T19:36:40.357076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.6 Inference","metadata":{}},{"cell_type":"code","source":"%cd ..","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.864911Z","iopub.status.idle":"2023-08-16T15:12:04.865705Z","shell.execute_reply.started":"2023-08-16T15:12:04.865461Z","shell.execute_reply":"2023-08-16T15:12:04.865486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%cd ..\n!pwd\n!ls","metadata":{"execution":{"iopub.status.busy":"2023-08-16T19:36:43.614368Z","iopub.execute_input":"2023-08-16T19:36:43.614734Z","iopub.status.idle":"2023-08-16T19:36:45.677696Z","shell.execute_reply.started":"2023-08-16T19:36:43.614703Z","shell.execute_reply":"2023-08-16T19:36:45.676462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_prediction(model, img, threshold):\n    model.eval()\n    #print(\"Image for prediction : \",img )\n    preds = model(img)\n    #print(\"it is inside prediction\")\n    #print(\"Prediction : \", preds)\n    for id in range(len(preds)) :\n        idx_list = []\n\n        for idx, score in enumerate(preds[id]['scores']) :\n            if score > threshold : #select idx which meets the threshold\n                idx_list.append(idx)\n\n        preds[id]['boxes'] = preds[id]['boxes'][idx_list]\n        preds[id]['labels'] = preds[id]['labels'][idx_list]\n        preds[id]['scores'] = preds[id]['scores'][idx_list]\n\n\n    return preds","metadata":{"execution":{"iopub.status.busy":"2023-08-17T13:34:14.902241Z","iopub.execute_input":"2023-08-17T13:34:14.902648Z","iopub.status.idle":"2023-08-17T13:34:14.942645Z","shell.execute_reply.started":"2023-08-17T13:34:14.902614Z","shell.execute_reply":"2023-08-17T13:34:14.941611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_adj","metadata":{"execution":{"iopub.status.busy":"2023-08-16T19:36:51.708503Z","iopub.execute_input":"2023-08-16T19:36:51.709666Z","iopub.status.idle":"2023-08-16T19:36:51.719769Z","shell.execute_reply.started":"2023-08-16T19:36:51.709621Z","shell.execute_reply":"2023-08-16T19:36:51.718586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.873787Z","iopub.status.idle":"2023-08-16T15:12:04.874699Z","shell.execute_reply.started":"2023-08-16T15:12:04.874418Z","shell.execute_reply":"2023-08-16T15:12:04.874444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"retina.load_state_dict(torch.load(f'retina_{num_epochs}.pt'))\n\nfrom tqdm import tqdm\n\nlabels = []\npreds_adj_all = []\nannot_all = []\n\nfor im, annot in tqdm(test_data_loader, position = 0, leave = True):\n    im = list(img.to(device) for img in im)\n    #print(\"Batch Index:\", batch_idx)\n    #annot = [{k: v.to(device) for k, v in t.items()} for t in annot]\n    #print(\"Test Image :\" , im)\n    #print(\" Test Loader : \",annot )\n    \n    # Move annotations to CPU\n    annot = [{k: v.to(torch.device('cpu')) for k, v in t.items()} for t in annot]\n    \n    for t in annot:\n        labels += t['labels']\n\n    with torch.no_grad():\n        preds_adj = make_prediction(retina, im, 0.4)\n        #print(\"Test Image Prediction :\" , preds_adj)\n        #devType = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n        preds_adj = [{k: v.to(torch.device('cpu')) for k, v in t.items()} for t in preds_adj]\n        \n                \n        preds_adj_all.append(preds_adj)\n        annot_all.append(annot)\n        #print(\" Test Loader annot all: \",annot_all )\n        #print(\" Test Loader : \",len(annot) )\n        print(preds_adj)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T19:36:56.332838Z","iopub.execute_input":"2023-08-16T19:36:56.333579Z","iopub.status.idle":"2023-08-16T19:37:15.927157Z","shell.execute_reply.started":"2023-08-16T19:36:56.333544Z","shell.execute_reply":"2023-08-16T19:37:15.926153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_adj_all","metadata":{"execution":{"iopub.status.busy":"2023-08-16T19:18:24.427178Z","iopub.execute_input":"2023-08-16T19:18:24.427587Z","iopub.status.idle":"2023-08-16T19:18:24.656731Z","shell.execute_reply.started":"2023-08-16T19:18:24.427555Z","shell.execute_reply":"2023-08-16T19:18:24.655803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nrows = 8\nncols = 2\nfig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols*4, nrows*4))\n\nbatch_i = 0\nfor im, annot in test_data_loader:\n    pos = batch_i * 4 + 1\n    for sample_i in range(len(im)) :\n        print(im[sample_i])\n        print(annot[sample_i])\n        img, rects = plot_image_from_output(im[sample_i], annot[sample_i])\n        axes[(pos)//2, 1-((pos)%2)].imshow(img)\n        for rect in rects:\n            axes[(pos)//2, 1-((pos)%2)].add_patch(rect)\n        \n        img, rects = plot_image_from_output(im[sample_i], preds_adj_all[batch_i][sample_i])\n        axes[(pos)//2, 1-((pos+1)%2)].imshow(img)\n        for rect in rects:\n            axes[(pos)//2, 1-((pos+1)%2)].add_patch(rect)\n\n        pos += 2\n\n    batch_i += 1\n    if batch_i == 4:\n        break\n\n# remove xtick, ytick\nfor idx, ax in enumerate(axes.flat):\n    ax.set_xticks([])\n    ax.set_yticks([])\n\ncolnames = ['True', 'Pred']\n\nfor idx, ax in enumerate(axes[0]):\n    ax.set_title(colnames[idx])\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-16T19:18:39.397290Z","iopub.execute_input":"2023-08-16T19:18:39.398166Z","iopub.status.idle":"2023-08-16T19:18:40.888123Z","shell.execute_reply.started":"2023-08-16T19:18:39.398113Z","shell.execute_reply":"2023-08-16T19:18:40.887246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#preds_adj_all","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.882679Z","iopub.status.idle":"2023-08-16T15:12:04.883461Z","shell.execute_reply.started":"2023-08-16T15:12:04.883214Z","shell.execute_reply":"2023-08-16T15:12:04.883238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cpu') if torch.cuda.is_available() else torch.device('cpu')\n#retina.to(device)\n#device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.884846Z","iopub.status.idle":"2023-08-16T15:12:04.885655Z","shell.execute_reply.started":"2023-08-16T15:12:04.885404Z","shell.execute_reply":"2023-08-16T15:12:04.885429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## DO NOT CHANGE - CHAT\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\ndef plot_image_from_output_test(img, boxes, scores, threshold=0.5):\n    img = img.cpu().permute(1, 2, 0).numpy()\n    rects = []\n\n    for i in range(boxes.size(0)):\n        xmin, ymin, xmax, ymax = boxes[i]\n        score = scores[i].item()  # Convert the score tensor to a scalar\n        if score > threshold:  # Adjust the threshold as needed\n            rect = patches.Rectangle(\n                (xmin, ymin), xmax - xmin, ymax - ymin, linewidth=1, edgecolor='r', facecolor='none'\n            )\n            rects.append(rect)\n\n    return img, rects\n","metadata":{"execution":{"iopub.status.busy":"2023-08-16T19:19:00.035538Z","iopub.execute_input":"2023-08-16T19:19:00.036327Z","iopub.status.idle":"2023-08-16T19:19:00.045360Z","shell.execute_reply.started":"2023-08-16T19:19:00.036293Z","shell.execute_reply":"2023-08-16T19:19:00.044124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#preds_adj_all","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.889266Z","iopub.status.idle":"2023-08-16T15:12:04.890293Z","shell.execute_reply.started":"2023-08-16T15:12:04.890008Z","shell.execute_reply":"2023-08-16T15:12:04.890036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## DO NOT CHANGE - CHAT\n\n# WORKING ...ALTERNATE ROW\nnrows = 8\nncols = 2\nfig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols*4, nrows*4))\n\nbatch_i = 0\nfor im, annot in test_data_loader:\n    pos = batch_i * 4 + 1\n    #pos = batch_i * (nrows // 2) * ncols + sample_i * ncols + 1  # Calculate the correct position\n    im_cpu = tuple(t.cpu() for t in im)\n    boxes_cpu = annot[0]['boxes'].cpu()\n    labels_cpu = annot[0]['labels'].cpu()\n    \n    preds_adj_all_cpu = []\n    for preds_adj_dict in preds_adj_all[batch_i]:\n        preds_adj_cpu = {\n            'boxes': preds_adj_dict['boxes'].cpu(),\n            'scores': preds_adj_dict['scores'].cpu()\n        }\n        preds_adj_all_cpu.append(preds_adj_cpu)\n\n    for sample_i in range(len(im_cpu)):\n        img, rects = plot_image_from_output_test(im_cpu[sample_i], annot[sample_i]['boxes'],annot[sample_i]['labels'],) #preds_adj_all_cpu[sample_i]['boxes'], preds_adj_all_cpu[sample_i]['scores'])\n        print(im_cpu ,annot[sample_i]['boxes'], annot[sample_i]['labels'] )\n        axes[(pos)//2, 1-((pos)%2)].imshow(img)\n        \n        for rect in rects:\n            axes[(pos)//2, 1-((pos)%2)].add_patch(rect)\n        \n        img, rects = plot_image_from_output_test(im_cpu[sample_i], preds_adj_all_cpu[sample_i]['boxes'], preds_adj_all_cpu[sample_i]['scores'])\n        axes[(pos)//2, 1-((pos+1)%2)].imshow(img)\n        \n        for rect in rects:\n            axes[(pos)//2, 1-((pos+1)%2)].add_patch(rect)\n\n        pos += 1\n\n    batch_i += 1\n    if batch_i == 4:\n        break\n\n# remove xtick, ytick\nfor idx, ax in enumerate(axes.flat):\n    ax.set_xticks([])\n    ax.set_yticks([])\n\ncolnames = ['True', 'Pred']\n\nfor idx, ax in enumerate(axes[0]):\n    ax.set_title(colnames[idx])\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.891768Z","iopub.status.idle":"2023-08-16T15:12:04.892585Z","shell.execute_reply.started":"2023-08-16T15:12:04.892328Z","shell.execute_reply":"2023-08-16T15:12:04.892352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_image_from_output_test(img, boxes, scores, threshold=0.5):\n    img = img.cpu().permute(1, 2, 0).numpy()\n    rects = []\n    \n    for i in range(boxes.size(0)):\n        xmin, ymin, xmax, ymax = boxes[i]\n        score = scores[i].item() if isinstance(scores, torch.Tensor) else scores\n        if score > threshold:\n            rect = patches.Rectangle(\n                (xmin, ymin), xmax - xmin, ymax - ymin, linewidth=1, edgecolor='r', facecolor='none'\n            )\n            rects.append(rect)\n    \n    return img, rects\n\nnrows = 8\nncols = 2\nfig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols*4, nrows*4))\n\nbatch_i = 0\nfor im, annot in test_data_loader:\n    pos = batch_i * 4\n    \n    im_cpu = tuple(t.cpu() for t in im)\n    boxes_cpu = annot[0]['boxes'].cpu()\n    labels_cpu = annot[0]['labels'].cpu()\n\n    preds_adj_all_cpu = []\n    for preds_adj_dict in preds_adj_all[batch_i]:\n        preds_adj_cpu = {\n            'boxes': preds_adj_dict['boxes'].cpu(),\n            'scores': preds_adj_dict['scores'].cpu()\n        }\n        preds_adj_all_cpu.append(preds_adj_cpu)\n\n    for sample_i in range(len(im_cpu)):\n        img_true, rects_true = plot_image_from_output_test(im_cpu[sample_i], boxes_cpu, 1.0)  # Use a placeholder score\n        axes[pos // ncols, pos % ncols].imshow(img_true)\n\n        for rect in rects_true:\n            axes[pos // ncols, pos % ncols].add_patch(rect)\n        \n        if sample_i < len(preds_adj_all_cpu):\n            img_pred, rects_pred = plot_image_from_output_test(im_cpu[sample_i], preds_adj_all_cpu[sample_i]['boxes'], preds_adj_all_cpu[sample_i]['scores'])\n            axes[(pos + 1) // ncols, (pos + 1) % ncols].imshow(img_pred)\n\n            for rect in rects_pred:\n                axes[(pos + 1) // ncols, (pos + 1) % ncols].add_patch(rect)\n\n        pos += 2\n\n    batch_i += 1\n    if batch_i == 4:\n        break\n\n# Remove xtick, ytick\nfor idx, ax in enumerate(axes.flat):\n    ax.set_xticks([])\n    ax.set_yticks([])\n\ncolnames = ['True', 'Pred']\nfor idx, ax in enumerate(axes[0]):\n    ax.set_title(colnames[idx])\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.894343Z","iopub.status.idle":"2023-08-16T15:12:04.895201Z","shell.execute_reply.started":"2023-08-16T15:12:04.894946Z","shell.execute_reply":"2023-08-16T15:12:04.894970Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_image_from_output_test(img, boxes, scores, threshold=0.5):\n    img = img.cpu().permute(1, 2, 0).numpy()\n    rects = []\n    \n    for i in range(boxes.size(0)):\n        xmin, ymin, xmax, ymax = boxes[i]\n        score = scores[i].item() if isinstance(scores, torch.Tensor) else scores\n        if score > threshold:\n            rect = patches.Rectangle(\n                (xmin, ymin), xmax - xmin, ymax - ymin, linewidth=1, edgecolor='r', facecolor='none'\n            )\n            rects.append(rect)\n    \n    return img, rects\n\nnrows = 8\nncols = 2\nfig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols*4, nrows*4))\n\nbatch_i = 0\nfor im, annot in test_data_loader:\n    pos = batch_i * 2\n    \n    im_cpu = tuple(t.cpu() for t in im)\n    boxes_cpu = annot[0]['boxes'].cpu()\n    labels_cpu = annot[0]['labels'].cpu()\n\n    preds_adj_all_cpu = []\n    for preds_adj_dict in preds_adj_all[batch_i]:\n        preds_adj_cpu = {\n            'boxes': preds_adj_dict['boxes'].cpu(),\n            'scores': preds_adj_dict['scores'].cpu()\n        }\n        preds_adj_all_cpu.append(preds_adj_cpu)\n\n    for sample_i in range(len(im_cpu)):\n        img_true, rects_true = plot_image_from_output_test(im_cpu[sample_i], boxes_cpu, 1.0)  # Use a placeholder score\n        axes[pos // ncols, pos % ncols].imshow(img_true)\n\n        for rect in rects_true:\n            axes[pos // ncols, pos % ncols].add_patch(rect)\n        \n        if sample_i < len(preds_adj_all_cpu):\n            img_pred, rects_pred = plot_image_from_output_test(im_cpu[sample_i], preds_adj_all_cpu[sample_i]['boxes'], preds_adj_all_cpu[sample_i]['scores'])\n            axes[(pos + 1) // ncols, (pos + 1) % ncols].imshow(img_pred)\n\n            for rect in rects_pred:\n                axes[(pos + 1) // ncols, (pos + 1) % ncols].add_patch(rect)\n\n        pos += 2\n\n    batch_i += 1\n    if batch_i == 4:\n        break\n\n# Remove xtick, ytick\nfor idx, ax in enumerate(axes.flat):\n    ax.set_xticks([])\n    ax.set_yticks([])\n\ncolnames = ['True', 'Pred']\nfor idx, ax in enumerate(axes[0]):\n    ax.set_title(colnames[idx])\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.896616Z","iopub.status.idle":"2023-08-16T15:12:04.897393Z","shell.execute_reply.started":"2023-08-16T15:12:04.897150Z","shell.execute_reply":"2023-08-16T15:12:04.897174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_image_from_output_test(img, boxes, scores, threshold=0.5):\n    img = img.cpu().permute(1, 2, 0).numpy()\n    rects = []\n    \n    if isinstance(scores, torch.Tensor):\n        scores = scores.cpu().numpy()\n        for i in range(boxes.size(0)):\n            xmin, ymin, xmax, ymax = boxes[i]\n            score = scores[i].item() if len(scores) > i else 0.0\n            if score > threshold:\n                rect = patches.Rectangle(\n                    (xmin, ymin), xmax - xmin, ymax - ymin, linewidth=1, edgecolor='r', facecolor='none'\n                )\n                rects.append(rect)\n    else:\n        if scores > threshold:\n            xmin, ymin, xmax, ymax = boxes\n            rect = patches.Rectangle(\n                (xmin, ymin), xmax - xmin, ymax - ymin, linewidth=1, edgecolor='r', facecolor='none'\n            )\n            rects.append(rect)\n    \n    return img, rects\n","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.898795Z","iopub.status.idle":"2023-08-16T15:12:04.899597Z","shell.execute_reply.started":"2023-08-16T15:12:04.899339Z","shell.execute_reply":"2023-08-16T15:12:04.899363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nrows = 8\nncols = 2\nfig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols*4, nrows*4))\n\nbatch_i = 0\nfor im, annot in test_data_loader:\n    pos = batch_i * 4\n    \n    im_cpu = tuple(t.cpu() for t in im)\n    boxes_cpu = annot[0]['boxes'].cpu()\n    labels_cpu = annot[0]['labels'].cpu()\n\n    preds_adj_all_cpu = []\n    for preds_adj_dict in preds_adj_all[batch_i]:\n        preds_adj_cpu = {\n            'boxes': preds_adj_dict['boxes'].cpu(),\n            'scores': preds_adj_dict['scores'].cpu()\n        }\n        preds_adj_all_cpu.append(preds_adj_cpu)\n\n    for sample_i in range(len(im_cpu)):\n        true_boxes = boxes_cpu[sample_i].numpy()\n        true_labels = labels_cpu[sample_i].numpy()\n        \n        img_true, rects_true = plot_image_from_output_test(im_cpu[sample_i], boxes_cpu, labels_cpu[sample_i])\n        axes[pos // ncols, pos % ncols].imshow(img_true)\n\n        for rect in rects_true:\n            axes[pos // ncols, pos % ncols].add_patch(rect)\n        \n        if sample_i < len(preds_adj_all_cpu):\n            pred_boxes = preds_adj_all_cpu[sample_i]['boxes'].numpy()\n            pred_scores = preds_adj_all_cpu[sample_i]['scores'].numpy()\n            \n            img_pred, rects_pred = plot_image_from_output_test(im_cpu[sample_i], preds_adj_all_cpu[sample_i]['boxes'], preds_adj_all_cpu[sample_i]['scores'])\n            axes[(pos + 1) // ncols, (pos + 1) % ncols].imshow(img_pred)\n\n            for rect in rects_pred:\n                axes[(pos + 1) // ncols, (pos + 1) % ncols].add_patch(rect)\n\n            # Print image ID, true bounding boxes, and predicted boxes\n            print(f\"Image ID: {sample_i}\")\n            print(\"True Bounding Boxes:\")\n            for i, box in enumerate(true_boxes):\n                print(f\"  True Box {i + 1}: {box}, Label: {true_labels[i]}\")\n            print(\"Predicted Bounding Boxes:\")\n            for i, box in enumerate(pred_boxes):\n                print(f\"  Predicted Box {i + 1}: {box}, Score: {pred_scores[i]}\")\n            print(\"=\" * 30)  # Separator between images\n        \n        pos += 2\n\n    batch_i += 1\n    if batch_i == 4:\n        break\n\n# Remove xtick, ytick\nfor idx, ax in enumerate(axes.flat):\n    ax.set_xticks([])\n    ax.set_yticks([])\n\ncolnames = ['True', 'Pred']\nfor idx, ax in enumerate(axes[0]):\n    ax.set_title(colnames[idx])\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.901038Z","iopub.status.idle":"2023-08-16T15:12:04.901849Z","shell.execute_reply.started":"2023-08-16T15:12:04.901604Z","shell.execute_reply":"2023-08-16T15:12:04.901628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/Pseudo-Lab/Tutorial-Book-Utils /kaggle/working/Tutorial-Book-Utils","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.903267Z","iopub.status.idle":"2023-08-16T15:12:04.904176Z","shell.execute_reply.started":"2023-08-16T15:12:04.903881Z","shell.execute_reply":"2023-08-16T15:12:04.903908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pwd","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.905784Z","iopub.status.idle":"2023-08-16T15:12:04.906652Z","shell.execute_reply.started":"2023-08-16T15:12:04.906374Z","shell.execute_reply":"2023-08-16T15:12:04.906399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install gdown","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.908079Z","iopub.status.idle":"2023-08-16T15:12:04.908898Z","shell.execute_reply.started":"2023-08-16T15:12:04.908642Z","shell.execute_reply":"2023-08-16T15:12:04.908665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python Tutorial-Book-Utils/PL_data_loader.py --data FaceMaskDetection","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.910311Z","iopub.status.idle":"2023-08-16T15:12:04.911120Z","shell.execute_reply.started":"2023-08-16T15:12:04.910855Z","shell.execute_reply":"2023-08-16T15:12:04.910880Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip -q Face\\ Mask\\ Detection.zip","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.912502Z","iopub.status.idle":"2023-08-16T15:12:04.913313Z","shell.execute_reply.started":"2023-08-16T15:12:04.913032Z","shell.execute_reply":"2023-08-16T15:12:04.913055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/Tutorial-Book-Utils\n!ls","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.914951Z","iopub.status.idle":"2023-08-16T15:12:04.915759Z","shell.execute_reply.started":"2023-08-16T15:12:04.915509Z","shell.execute_reply":"2023-08-16T15:12:04.915534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import utils_ObjectDetection as utils","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.917185Z","iopub.status.idle":"2023-08-16T15:12:04.917979Z","shell.execute_reply.started":"2023-08-16T15:12:04.917713Z","shell.execute_reply":"2023-08-16T15:12:04.917736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/Tutorial-Book-Utils\n\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nsample_metrics = []\nfor batch_i in range(len(preds_adj_all)):\n    sample_metrics += utils.get_batch_statistics(preds_adj_all[batch_i], annot_all[batch_i], iou_threshold=0.5) \n\ntrue_positives, pred_scores, pred_labels = [torch.cat(x, 0) for x in list(zip(*sample_metrics))]  # all the batches get concatenated\nprecision, recall, AP, f1, ap_class = utils.ap_per_class(true_positives, pred_scores, pred_labels, torch.tensor(labels))\n#mAP = torch.mean(AP)\n#print(f'mAP : {mAP}')\nprint(\"Pred Scores : \", pred_scores)\nprint(\"Precision : \", precision)\nmAP = torch.mean(AP.to(torch.float32))\nprint(f'mAP : {mAP.item()}')\nprint(f'AP : {AP}')","metadata":{"execution":{"iopub.status.busy":"2023-08-16T19:37:38.056779Z","iopub.execute_input":"2023-08-16T19:37:38.057515Z","iopub.status.idle":"2023-08-16T19:37:38.170910Z","shell.execute_reply.started":"2023-08-16T19:37:38.057478Z","shell.execute_reply":"2023-08-16T19:37:38.169840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_adj_all_cpu","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.921625Z","iopub.status.idle":"2023-08-16T15:12:04.922440Z","shell.execute_reply.started":"2023-08-16T15:12:04.922193Z","shell.execute_reply":"2023-08-16T15:12:04.922218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/Tutorial-Book-Utils\nimport utils_ObjectDetection as utils\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\nsample_metrics = []\nfor batch_i in range(len(preds_adj_all)):\n    preds_adj_all_cpu = [\n        {\n            'boxes': preds_adj_dict['boxes'].cpu(),\n            'labels': preds_adj_dict['labels'].cpu(),\n            'scores': preds_adj_dict['scores'].cpu()\n        }\n        for preds_adj_dict in preds_adj_all[batch_i]\n    ]\n    annot_all_cpu = [\n        {\n            'boxes': annot_dict['boxes'].cpu(),\n            'labels': annot_dict['labels'].cpu()\n        }\n        for annot_dict in annot_all[batch_i]\n    ]\n    sample_metrics += utils.get_batch_statistics(preds_adj_all_cpu, annot_all_cpu, iou_threshold=0.5)\n\ntrue_positives, pred_scores, pred_labels = [torch.cat(x, 0) for x in list(zip(*sample_metrics))]\nprecision, recall, AP, f1, ap_class = utils.ap_per_class(true_positives, pred_scores, pred_labels, torch.tensor(labels).cpu())\nmAP = torch.mean(AP.to(torch.float32))\nprint(f'mAP : {mAP.item()}')\nprint(f'AP : {AP}')\n","metadata":{"execution":{"iopub.status.busy":"2023-08-16T15:12:04.923855Z","iopub.status.idle":"2023-08-16T15:12:04.924654Z","shell.execute_reply.started":"2023-08-16T15:12:04.924408Z","shell.execute_reply":"2023-08-16T15:12:04.924433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}