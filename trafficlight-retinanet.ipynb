{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5794045,"sourceType":"datasetVersion","datasetId":3328159}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://debuggercafe.com/traffic-light-detection-using-retinanet/","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config.py","metadata":{}},{"cell_type":"code","source":"import torch\n\nBATCH_SIZE = 8 # Increase / decrease according to GPU memeory.\nRESIZE_TO = 640 # Resize the image for training and transforms.\nNUM_EPOCHS = 75 # Number of epochs to train for.\nNUM_WORKERS = 4 # Number of parallel workers for data loading.\n\nDEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# Training images and XML files directory.\nTRAIN_IMG = 'input/S2TLD_720x1280/normal_2/JPEGImages'\nTRAIN_ANNOT = 'input/S2TLD_720x1280/normal_2/Annotations'\n# Validation images and XML files directory.\nVALID_IMG = 'input/S2TLD_720x1280/normal_1/JPEGImages'\nVALID_ANNOT = 'input/S2TLD_720x1280/normal_1/Annotations'\n\n# Classes: 0 index is reserved for background.\nCLASSES = [\n    '__background__', 'red', 'yellow', 'green', 'off'\n]\n\nNUM_CLASSES = len(CLASSES)\n\n# Whether to visualize images after crearing the data loaders.\nVISUALIZE_TRANSFORMED_IMAGES = False\n\n# Location to save model and plots.\nOUT_DIR = 'outputs'","metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:49:16.086637Z","iopub.execute_input":"2023-10-06T16:49:16.087026Z","iopub.status.idle":"2023-10-06T16:49:16.093874Z","shell.execute_reply.started":"2023-10-06T16:49:16.086997Z","shell.execute_reply":"2023-10-06T16:49:16.092558Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# Custom utils.py","metadata":{}},{"cell_type":"code","source":"import albumentations as A\nimport cv2\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\n\nfrom albumentations.pytorch import ToTensorV2\n#from config import DEVICE, CLASSES\n\nplt.style.use('ggplot')\n\n# This class keeps track of the training and validation loss values\n# and helps to get the average for each epoch as well.\nclass Averager:\n    def __init__(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n        \n    def send(self, value):\n        self.current_total += value\n        self.iterations += 1\n    \n    @property\n    def value(self):\n        if self.iterations == 0:\n            return 0\n        else:\n            return 1.0 * self.current_total / self.iterations\n    \n    def reset(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n\nclass SaveBestModel:\n    \"\"\"\n    Class to save the best model while training. If the current epoch's \n    validation mAP @0.5:0.95 IoU higher than the previous highest, then save the\n    model state.\n    \"\"\"\n    def __init__(\n        self, best_valid_map=float(0)\n    ):\n        self.best_valid_map = best_valid_map\n        \n    def __call__(\n        self, \n        model, \n        current_valid_map, \n        epoch, \n        OUT_DIR,\n    ):\n        if current_valid_map > self.best_valid_map:\n            self.best_valid_map = current_valid_map\n            print(f\"\\nBEST VALIDATION mAP: {self.best_valid_map}\")\n            print(f\"\\nSAVING BEST MODEL FOR EPOCH: {epoch+1}\\n\")\n            torch.save({\n                'epoch': epoch+1,\n                'model_state_dict': model.state_dict(),\n                }, f\"{OUT_DIR}/best_model.pth\")\n\ndef collate_fn(batch):\n    \"\"\"\n    To handle the data loading as different images may have different number \n    of objects and to handle varying size tensors as well.\n    \"\"\"\n    return tuple(zip(*batch))\n\n# Define the training tranforms.\ndef get_train_transform():\n    return A.Compose([\n        A.HorizontalFlip(p=0.5),\n        A.Rotate(limit=30, p=0.3),\n        A.RandomBrightnessContrast(p=0.3),\n        A.RandomGamma(p=0.3),\n        A.RandomFog(fog_coef_lower=0.1, fog_coef_upper=0.3, p=0.3),\n        ToTensorV2(p=1.0),\n    ], bbox_params={\n        'format': 'pascal_voc',\n        'label_fields': ['labels']\n    })\n\n# Define the validation transforms.\ndef get_valid_transform():\n    return A.Compose([\n        ToTensorV2(p=1.0),\n    ], bbox_params={\n        'format': 'pascal_voc', \n        'label_fields': ['labels']\n    })\n\n\ndef show_tranformed_image(train_loader):\n    \"\"\"\n    This function shows the transformed images from the `train_loader`.\n    Helps to check whether the tranformed images along with the corresponding\n    labels are correct or not.\n    Only runs if `VISUALIZE_TRANSFORMED_IMAGES = True` in config.py.\n    \"\"\"\n    if len(train_loader) > 0:\n        for i in range(1):\n            images, targets = next(iter(train_loader))\n            images = list(image.to(DEVICE) for image in images)\n            targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n            boxes = targets[i]['boxes'].cpu().numpy().astype(np.int32)\n            labels = targets[i]['labels'].cpu().numpy().astype(np.int32)\n            sample = images[i].permute(1, 2, 0).cpu().numpy()\n            sample = cv2.cvtColor(sample, cv2.COLOR_RGB2BGR)\n            for box_num, box in enumerate(boxes):\n                cv2.rectangle(sample,\n                            (box[0], box[1]),\n                            (box[2], box[3]),\n                            (0, 0, 255), 2)\n                cv2.putText(sample, CLASSES[labels[box_num]], \n                            (box[0], box[1]-10), cv2.FONT_HERSHEY_SIMPLEX, \n                            1.0, (0, 0, 255), 2)\n            cv2.imshow('Transformed image', sample)\n            cv2.waitKey(0)\n            cv2.destroyAllWindows()\n\ndef save_model(epoch, model, optimizer):\n    \"\"\"\n    Function to save the trained model till current epoch, or whenver called\n    \"\"\"\n    torch.save({\n                'epoch': epoch+1,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                }, 'outputs/last_model.pth')\n\ndef save_loss_plot(\n    OUT_DIR, \n    train_loss_list, \n    x_label='iterations',\n    y_label='train loss',\n    save_name='train_loss'\n):\n    \"\"\"\n    Function to save both train loss graph.\n    \n    :param OUT_DIR: Path to save the graphs.\n    :param train_loss_list: List containing the training loss values.\n    \"\"\"\n    figure_1 = plt.figure(figsize=(10, 7), num=1, clear=True)\n    train_ax = figure_1.add_subplot()\n    train_ax.plot(train_loss_list, color='tab:blue')\n    train_ax.set_xlabel(x_label)\n    train_ax.set_ylabel(y_label)\n    figure_1.savefig(f\"{OUT_DIR}/{save_name}.png\")\n    print('SAVING PLOTS COMPLETE...')\n\ndef save_mAP(OUT_DIR, map_05, map):\n    \"\"\"\n    Saves the mAP@0.5 and mAP@0.5:0.95 per epoch.\n    :param OUT_DIR: Path to save the graphs.\n    :param map_05: List containing mAP values at 0.5 IoU.\n    :param map: List containing mAP values at 0.5:0.95 IoU.\n    \"\"\"\n    figure = plt.figure(figsize=(10, 7), num=1, clear=True)\n    ax = figure.add_subplot()\n    ax.plot(\n        map_05, color='tab:orange', linestyle='-', \n        label='mAP@0.5'\n    )\n    ax.plot(\n        map, color='tab:red', linestyle='-', \n        label='mAP@0.5:0.95'\n    )\n    ax.set_xlabel('Epochs')\n    ax.set_ylabel('mAP')\n    ax.legend()\n    figure.savefig(f\"{OUT_DIR}/map.png\")","metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:49:18.408617Z","iopub.execute_input":"2023-10-06T16:49:18.409304Z","iopub.status.idle":"2023-10-06T16:49:18.430534Z","shell.execute_reply.started":"2023-10-06T16:49:18.409268Z","shell.execute_reply":"2023-10-06T16:49:18.429288Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Datasets.py","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import torch\nimport cv2\nimport numpy as np\nimport os\nimport glob as glob\n\nfrom xml.etree import ElementTree as et\nfrom torch.utils.data import Dataset, DataLoader\n#from custom_utils import collate_fn, get_train_transform, get_valid_transform\n\n# The dataset class.\nclass CustomDataset(Dataset):\n    def __init__(\n            self, \n            img_path, \n            annot_path, \n            width, \n            height, \n            classes, \n            transforms=None\n    ):\n        self.transforms = transforms\n        self.img_path = img_path\n        self.annot_path = annot_path\n        self.height = height\n        self.width = width\n        self.classes = classes\n        self.image_file_types = ['*.jpg', '*.jpeg', '*.png', '*.ppm', '*.JPG']\n        self.all_image_paths = []\n        \n        # Get all the image paths in sorted order.\n        for file_type in self.image_file_types:\n            self.all_image_paths.extend(glob.glob(os.path.join(self.img_path, file_type)))\n        self.all_images = [image_path.split(os.path.sep)[-1] for image_path in self.all_image_paths]\n        self.all_images = sorted(self.all_images)\n\n    def __getitem__(self, idx):\n        # Capture the image name and the full image path.\n        image_name = self.all_images[idx]\n        image_path = os.path.join(self.img_path, image_name)\n\n        # Read and preprocess the image.\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image_resized = cv2.resize(image, (self.width, self.height))\n        image_resized /= 255.0\n        \n        # Capture the corresponding XML file for getting the annotations.\n        annot_filename = os.path.splitext(image_name)[0] + '.xml'\n        annot_file_path = os.path.join(self.annot_path, annot_filename)\n        \n        boxes = []\n        labels = []\n        tree = et.parse(annot_file_path)\n        root = tree.getroot()\n        \n        # Original image width and height.\n        image_width = image.shape[1]\n        image_height = image.shape[0]\n        \n        # Box coordinates for xml files are extracted \n        # and corrected for image size given.\n        for member in root.findall('object'):\n            # Get label and map the `classes`.\n            labels.append(self.classes.index(member.find('name').text))\n            \n            # Left corner x-coordinates.\n            xmin = int(member.find('bndbox').find('xmin').text)\n            # Right corner x-coordinates.\n            xmax = int(member.find('bndbox').find('xmax').text)\n            # Left corner y-coordinates.\n            ymin = int(member.find('bndbox').find('ymin').text)\n            # Right corner y-coordinates.\n            ymax = int(member.find('bndbox').find('ymax').text)\n            \n            # Resize the bounding boxes according \n            # to resized image `width`, `height`.\n            xmin_final = (xmin/image_width)*self.width\n            xmax_final = (xmax/image_width)*self.width\n            ymin_final = (ymin/image_height)*self.height\n            ymax_final = (ymax/image_height)*self.height\n\n            # Check that max coordinates are at least one pixel\n            # larger than min coordinates.\n            if xmax_final == xmin_final:\n                xmin_final -= 1\n            if ymax_final == ymin_final:\n                ymin_final -= 1\n            # Check that all coordinates are within the image.\n            if xmax_final > self.width:\n                xmax_final = self.width\n            if ymax_final > self.height:\n                ymax_final = self.height\n            \n            boxes.append([xmin_final, ymin_final, xmax_final, ymax_final])\n        \n        # Bounding box to tensor.\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        # Area of the bounding boxes.\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) if len(boxes) > 0 \\\n            else torch.as_tensor(boxes, dtype=torch.float32)\n        # No crowd instances.\n        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n        # Labels to tensor.\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n\n        # Prepare the final `target` dictionary.\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n        image_id = torch.tensor([idx])\n        target[\"image_id\"] = image_id\n\n        # Apply the image transforms.\n        if self.transforms:\n            sample = self.transforms(image = image_resized,\n                                     bboxes = target['boxes'],\n                                     labels = labels)\n            image_resized = sample['image']\n            target['boxes'] = torch.Tensor(sample['bboxes'])\n        \n        if np.isnan((target['boxes']).numpy()).any() or target['boxes'].shape == torch.Size([0]):\n            target['boxes'] = torch.zeros((0, 4), dtype=torch.int64)\n        return image_resized, target\n\n    def __len__(self):\n        return len(self.all_images)\n\n# Prepare the final datasets and data loaders.\ndef create_train_dataset(img_dir, annot_dir, classes, resize=640):\n    train_dataset = CustomDataset(\n        img_dir, \n        annot_dir, \n        resize, \n        resize, \n        classes, \n        get_train_transform()\n    )\n    return train_dataset\ndef create_valid_dataset(img_dir, annot_dir, classes, resize=640):\n    valid_dataset = CustomDataset(\n        img_dir, \n        annot_dir, \n        resize, \n        resize, \n        classes, \n        get_valid_transform()\n    )\n    return valid_dataset\ndef create_train_loader(train_dataset, batch_size, num_workers=0):\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        collate_fn=collate_fn,\n        drop_last=False\n    )\n    return train_loader\ndef create_valid_loader(valid_dataset, batch_size, num_workers=0):\n    valid_loader = DataLoader(\n        valid_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        collate_fn=collate_fn,\n        drop_last=False\n    )\n    return valid_loader\n\n\n# execute `datasets.py` using Python command from \n# Terminal to visualize sample images\n# USAGE: python datasets.py\nif __name__ == '__main__':\n    #from config import (\n        #CLASSES, RESIZE_TO, TRAIN_IMG, TRAIN_ANNOT\n    #)   \n    # sanity check of the Dataset pipeline with sample visualization\n    dataset = CustomDataset(\n        TRAIN_IMG, \n        TRAIN_ANNOT, \n        RESIZE_TO, \n        RESIZE_TO, \n        CLASSES,\n        get_train_transform()\n    )\n    print(f\"Number of training images: {len(dataset)}\")\n    \n    # function to visualize a single sample\n    def visualize_sample(image, target):\n        image = np.array(image).transpose((1, 2, 0))\n        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n        for box_num in range(len(target['boxes'])):\n            box = target['boxes'][box_num]\n            label = CLASSES[target['labels'][box_num]]\n            cv2.rectangle(\n                image, \n                (int(box[0]), int(box[1])), (int(box[2]), int(box[3])),\n                (0, 0, 255), \n                2\n            )\n            cv2.putText(\n                image, \n                label, \n                (int(box[0]), int(box[1]-5)), \n                cv2.FONT_HERSHEY_SIMPLEX, \n                0.7, \n                (0, 0, 255), \n                2\n            )\n        cv2.imshow('Image', image)\n        cv2.waitKey(0)\n        \n    NUM_SAMPLES_TO_VISUALIZE = 50\n    for i in range(NUM_SAMPLES_TO_VISUALIZE):\n        image, target = dataset[i]\n        visualize_sample(image, target)","metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:49:26.482443Z","iopub.execute_input":"2023-10-06T16:49:26.482785Z","iopub.status.idle":"2023-10-06T16:49:26.616893Z","shell.execute_reply.started":"2023-10-06T16:49:26.482757Z","shell.execute_reply":"2023-10-06T16:49:26.615778Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Number of training images: 0\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[19], line 219\u001b[0m\n\u001b[1;32m    217\u001b[0m NUM_SAMPLES_TO_VISUALIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_SAMPLES_TO_VISUALIZE):\n\u001b[0;32m--> 219\u001b[0m     image, target \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    220\u001b[0m     visualize_sample(image, target)\n","Cell \u001b[0;32mIn[19], line 39\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# Capture the image name and the full image path.\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m     image_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_images\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     40\u001b[0m     image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_path, image_name)\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# Read and preprocess the image.\u001b[39;00m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"],"ename":"IndexError","evalue":"list index out of range","output_type":"error"}]},{"cell_type":"markdown","source":"# Model.py","metadata":{}},{"cell_type":"code","source":"import torchvision\nimport torch\n\nfrom functools import partial\nfrom torchvision.models.detection import RetinaNet_ResNet50_FPN_V2_Weights\nfrom torchvision.models.detection.retinanet import RetinaNetClassificationHead\n\ndef create_model(num_classes=91):\n    model = torchvision.models.detection.retinanet_resnet50_fpn_v2(\n        weights='DEFAULT'\n    )\n    num_anchors = model.head.classification_head.num_anchors\n\n    model.head.classification_head = RetinaNetClassificationHead(\n        in_channels=256,\n        num_anchors=num_anchors,\n        num_classes=num_classes,\n        norm_layer=partial(torch.nn.GroupNorm, 32)\n    )\n    return model\n\nif __name__ == '__main__':\n    model = create_model(4)\n    print(model)\n    # Total parameters and trainable parameters.\n    total_params = sum(p.numel() for p in model.parameters())\n    print(f\"{total_params:,} total parameters.\")\n    total_trainable_params = sum(\n        p.numel() for p in model.parameters() if p.requires_grad)\n    print(f\"{total_trainable_params:,} training parameters.\")","metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:48:15.955433Z","iopub.execute_input":"2023-10-06T16:48:15.955774Z","iopub.status.idle":"2023-10-06T16:48:26.579877Z","shell.execute_reply.started":"2023-10-06T16:48:15.955747Z","shell.execute_reply":"2023-10-06T16:48:26.578741Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/retinanet_resnet50_fpn_v2_coco-5905b1c5.pth\" to /root/.cache/torch/hub/checkpoints/retinanet_resnet50_fpn_v2_coco-5905b1c5.pth\n100%|██████████| 146M/146M [00:08<00:00, 18.1MB/s] \n","output_type":"stream"},{"name":"stdout","text":"RetinaNet(\n  (backbone): BackboneWithFPN(\n    (body): IntermediateLayerGetter(\n      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n      (layer1): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer2): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer3): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (4): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (5): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer4): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n      )\n    )\n    (fpn): FeaturePyramidNetwork(\n      (inner_blocks): ModuleList(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (2): Conv2dNormActivation(\n          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (layer_blocks): ModuleList(\n        (0-2): 3 x Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n      (extra_blocks): LastLevelP6P7(\n        (p6): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (p7): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      )\n    )\n  )\n  (anchor_generator): AnchorGenerator()\n  (head): RetinaNetHead(\n    (classification_head): RetinaNetClassificationHead(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n          (2): ReLU(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n          (2): ReLU(inplace=True)\n        )\n        (2): Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n          (2): ReLU(inplace=True)\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n          (2): ReLU(inplace=True)\n        )\n      )\n      (cls_logits): Conv2d(256, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    )\n    (regression_head): RetinaNetRegressionHead(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n          (2): ReLU(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n          (2): ReLU(inplace=True)\n        )\n        (2): Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n          (2): ReLU(inplace=True)\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n          (2): ReLU(inplace=True)\n        )\n      )\n      (bbox_reg): Conv2d(256, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    )\n  )\n  (transform): GeneralizedRCNNTransform(\n      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n  )\n)\n36,394,120 total parameters.\n36,168,776 training parameters.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Train.py","metadata":{}},{"cell_type":"code","source":"#from config import (\n    #DEVICE, \n    #NUM_CLASSES,    NUM_EPOCHS,     OUT_DIR,    VISUALIZE_TRANSFORMED_IMAGES,     NUM_WORKERS,    TRAIN_IMG,    TRAIN_ANNOT,\n    #VALID_IMG,    VALID_ANNOT,    CLASSES,    RESIZE_TO,    BATCH_SIZE)\n    \n#from model import create_model\n#from custom_utils import (\n    #Averager, \n    #SaveBestModel, \n    #save_model, \n    #save_loss_plot,\n    #save_mAP\n#)\nfrom tqdm.auto import tqdm\n#from datasets import (\n    #create_train_dataset,     create_valid_dataset,     create_train_loader,     create_valid_loader)\n    \nfrom torchmetrics.detection.mean_ap import MeanAveragePrecision\nfrom torch.optim.lr_scheduler import StepLR\n\nimport torch\nimport matplotlib.pyplot as plt\nimport time\nimport os\nimport numpy as np\nimport random\n\nplt.style.use('ggplot')\n\nseed = 42\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\nnp.random.seed(seed)\nrandom.seed(seed)\n\n# Function for running training iterations.\ndef train(train_data_loader, model):\n    print('Training')\n    model.train()\n    \n     # initialize tqdm progress bar\n    prog_bar = tqdm(\n        train_data_loader, \n        total=len(train_data_loader),\n        bar_format='{l_bar}{bar:20}{r_bar}{bar:-20b}'\n    )\n    \n    for i, data in enumerate(prog_bar):\n        optimizer.zero_grad()\n        images, targets = data\n        \n        images = list(image.to(DEVICE) for image in images)\n        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n        loss_dict = model(images, targets)\n\n        losses = sum(loss for loss in loss_dict.values())\n        loss_value = losses.item()\n\n        train_loss_hist.send(loss_value)\n\n        losses.backward()\n        optimizer.step()\n    \n        # update the loss value beside the progress bar for each iteration\n        prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}\")\n    return loss_value\n\n# Function for running validation iterations.\ndef validate(valid_data_loader, model):\n    print('Validating')\n    model.eval()\n    # Initialize tqdm progress bar.\n    prog_bar = tqdm(\n        valid_data_loader, \n        total=len(valid_data_loader),\n        bar_format='{l_bar}{bar:20}{r_bar}{bar:-20b}'\n    )\n    target = []\n    preds = []\n    for i, data in enumerate(prog_bar):\n        images, targets = data\n        \n        images = list(image.to(DEVICE) for image in images)\n        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n        \n        with torch.no_grad():\n            outputs = model(images, targets)\n\n        # For mAP calculation using Torchmetrics.\n        #####################################\n        for k in range(len(images)):\n            true_dict = dict()\n            preds_dict = dict()\n            true_dict['boxes'] = targets[k]['boxes'].detach()\n            true_dict['labels'] = targets[k]['labels'].detach()\n            preds_dict['boxes'] = outputs[k]['boxes'].detach()\n            preds_dict['scores'] = outputs[k]['scores'].detach()\n            preds_dict['labels'] = outputs[k]['labels'].detach()\n            preds.append(preds_dict)\n            target.append(true_dict)\n        #####################################\n\n    metric.reset()\n    metric.update(preds, target)\n    metric_summary = metric.compute()\n    return metric_summary\n\nif __name__ == '__main__':\n    os.makedirs(OUT_DIR, exist_ok=True)\n    train_dataset = create_train_dataset(\n        TRAIN_IMG, TRAIN_ANNOT, CLASSES, RESIZE_TO,\n    )\n    valid_dataset = create_valid_dataset(\n        VALID_IMG, VALID_ANNOT, CLASSES, RESIZE_TO\n    )\n    train_loader = create_train_loader(train_dataset, BATCH_SIZE, NUM_WORKERS)\n    valid_loader = create_valid_loader(valid_dataset, BATCH_SIZE, NUM_WORKERS)\n    print(f\"Number of training samples: {len(train_dataset)}\")\n    print(f\"Number of validation samples: {len(valid_dataset)}\\n\")\n\n    # Initialize the model and move to the computation device.\n    model = create_model(num_classes=NUM_CLASSES)\n    model = model.to(DEVICE)\n    print(model)\n    # Total parameters and trainable parameters.\n    total_params = sum(p.numel() for p in model.parameters())\n    print(f\"{total_params:,} total parameters.\")\n    total_trainable_params = sum(\n        p.numel() for p in model.parameters() if p.requires_grad)\n    print(f\"{total_trainable_params:,} training parameters.\")\n    params = [p for p in model.parameters() if p.requires_grad]\n    optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, nesterov=True)\n    scheduler = StepLR(\n        optimizer=optimizer, step_size=50, gamma=0.1, verbose=True\n    )\n\n    # To monitor training loss\n    train_loss_hist = Averager()\n    # To store training loss and mAP values.\n    train_loss_list = []\n    map_50_list = []\n    map_list = []\n\n    # Whether to show transformed images from data loader or not.\n    if VISUALIZE_TRANSFORMED_IMAGES:\n        from custom_utils import show_tranformed_image\n        show_tranformed_image(train_loader)\n\n    # To save best model.\n    save_best_model = SaveBestModel()\n\n    metric = MeanAveragePrecision()\n\n    # Training loop.\n    for epoch in range(NUM_EPOCHS):\n        print(f\"\\nEPOCH {epoch+1} of {NUM_EPOCHS}\")\n\n        # Reset the training loss histories for the current epoch.\n        train_loss_hist.reset()\n\n        # Start timer and carry out training and validation.\n        start = time.time()\n        train_loss = train(train_loader, model)\n        metric_summary = validate(valid_loader, model)\n        print(f\"Epoch #{epoch+1} train loss: {train_loss_hist.value:.3f}\")   \n        print(f\"Epoch #{epoch+1} mAP@0.50:0.95: {metric_summary['map']}\")\n        print(f\"Epoch #{epoch+1} mAP@0.50: {metric_summary['map_50']}\")   \n        end = time.time()\n        print(f\"Took {((end - start) / 60):.3f} minutes for epoch {epoch}\")\n\n        train_loss_list.append(train_loss)\n        map_50_list.append(metric_summary['map_50'])\n        map_list.append(metric_summary['map'])\n\n        # save the best model till now.\n        save_best_model(\n            model, float(metric_summary['map']), epoch, 'outputs'\n        )\n        # Save the current epoch model.\n        save_model(epoch, model, optimizer)\n\n        # Save loss plot.\n        save_loss_plot(OUT_DIR, train_loss_list)\n\n        # Save mAP plot.\n        save_mAP(OUT_DIR, map_50_list, map_list)\n        scheduler.step()","metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:48:53.420054Z","iopub.execute_input":"2023-10-06T16:48:53.420406Z","iopub.status.idle":"2023-10-06T16:49:05.828659Z","shell.execute_reply.started":"2023-10-06T16:48:53.420378Z","shell.execute_reply":"2023-10-06T16:49:05.826723Z"},"trusted":true},"execution_count":15,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 117\u001b[0m\n\u001b[1;32m    111\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m create_train_dataset(\n\u001b[1;32m    112\u001b[0m     TRAIN_IMG, TRAIN_ANNOT, CLASSES, RESIZE_TO,\n\u001b[1;32m    113\u001b[0m )\n\u001b[1;32m    114\u001b[0m valid_dataset \u001b[38;5;241m=\u001b[39m create_valid_dataset(\n\u001b[1;32m    115\u001b[0m     VALID_IMG, VALID_ANNOT, CLASSES, RESIZE_TO\n\u001b[1;32m    116\u001b[0m )\n\u001b[0;32m--> 117\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_train_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_WORKERS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m valid_loader \u001b[38;5;241m=\u001b[39m create_valid_loader(valid_dataset, BATCH_SIZE, NUM_WORKERS)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of training samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[9], line 153\u001b[0m, in \u001b[0;36mcreate_train_loader\u001b[0;34m(train_dataset, batch_size, num_workers)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_train_loader\u001b[39m(train_dataset, batch_size, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m--> 153\u001b[0m     train_loader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdrop_last\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_loader\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:351\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[0;32m--> 351\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    353\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/sampler.py:107\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement))\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue, but got num_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples))\n","\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"],"ename":"ValueError","evalue":"num_samples should be a positive integer value, but got num_samples=0","output_type":"error"}]},{"cell_type":"markdown","source":"# Eval.py","metadata":{}},{"cell_type":"code","source":"import torch\nimport argparse\n\nfrom tqdm import tqdm\n#from config import (    DEVICE,     NUM_CLASSES,     NUM_WORKERS,     RESIZE_TO,    CLASSES)\nfrom torchmetrics.detection.mean_ap import MeanAveragePrecision\n#from model import create_model\n#from datasets import create_valid_dataset, create_valid_loader\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\n    '--weights',\n    default='outputs/best_model.pth',\n    help='path to the model weights'\n)\nparser.add_argument(\n    '--input-images',\n    dest='input_images',\n    default='data/Test/Test/JPEGImages',\n    help='path to the evaluation images'\n)\nparser.add_argument(\n    '--input-annots',\n    dest='input_annots',\n    default='data/Test/Test/JPEGImages',\n    help='path to the evaluation annotations'\n)\nparser.add_argument(\n    '--batch',\n    default=8,\n    help='batch size for the data loader'\n)\nargs = parser.parse_args()\n\n# Evaluation function\ndef validate(valid_data_loader, model):\n    model.eval()\n    \n    # Initialize tqdm progress bar.\n    prog_bar = tqdm(valid_data_loader, total=len(valid_data_loader))\n    target = []\n    preds = []\n    for i, data in enumerate(prog_bar):\n        images, targets = data\n        \n        images = list(image.to(DEVICE) for image in images)\n        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n        \n        with torch.no_grad():\n            outputs = model(images, targets)\n\n        # For mAP calculation using Torchmetrics.\n        #####################################\n        for i in range(len(images)):\n            true_dict = dict()\n            preds_dict = dict()\n            true_dict['boxes'] = targets[i]['boxes'].detach()\n            true_dict['labels'] = targets[i]['labels'].detach()\n            preds_dict['boxes'] = outputs[i]['boxes'].detach()\n            preds_dict['scores'] = outputs[i]['scores'].detach()\n            preds_dict['labels'] = outputs[i]['labels'].detach()\n            preds.append(preds_dict)\n            target.append(true_dict)\n        #####################################\n\n    metric.reset()\n    metric.update(preds, target)\n    metric_summary = metric.compute()\n    return metric_summary\n\nif __name__ == '__main__':\n    # Load the best model and trained weights.\n    model = create_model(num_classes=NUM_CLASSES)\n    checkpoint = torch.load(args.weights, map_location=DEVICE)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model.to(DEVICE).eval()\n\n    test_dataset = create_valid_dataset(\n        args.input_images, \n        args.input_annots,\n        CLASSES,\n        RESIZE_TO\n    )\n    test_loader = create_valid_loader(\n        test_dataset, \n        args.batch,\n        num_workers=NUM_WORKERS,\n    )\n    metric = MeanAveragePrecision()\n\n    metric_summary = validate(test_loader, model)\n    print(f\"mAP_50: {metric_summary['map_50']*100:.3f}\")\n    print(f\"mAP_50_95: {metric_summary['map']*100:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:49:05.830058Z","iopub.status.idle":"2023-10-06T16:49:05.830697Z","shell.execute_reply.started":"2023-10-06T16:49:05.830511Z","shell.execute_reply":"2023-10-06T16:49:05.830537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference.py","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport torch\nimport glob as glob\nimport os\nimport time\nimport argparse\n\n#from model import create_model\nfrom torchvision import transforms as transforms\n#from config import (    NUM_CLASSES, DEVICE, CLASSES)\n\nnp.random.seed(42)\n\n# Construct the argument parser.\nparser = argparse.ArgumentParser()\nparser.add_argument(\n    '--weights',\n    default='outputs/best_model.pth',\n    help='path to the model weights'\n)\nparser.add_argument(\n    '-i', '--input', \n    help='path to input image directory',\n    required=True\n)\nparser.add_argument(\n    '--imgsz', \n    default=None,\n    type=int,\n    help='image resize shape'\n)\nparser.add_argument(\n    '--threshold',\n    default=0.25,\n    type=float,\n    help='detection threshold'\n)\nargs = parser.parse_args()\n\nos.makedirs('inference_outputs/images', exist_ok=True)\n\nCOLORS = [\n    [0, 0, 0],\n    [255, 0, 0],\n    [255, 255, 0],\n    [0, 255, 0],\n    [255, 255, 255]\n]\n\n# Load the best model and trained weights.\nmodel = create_model(num_classes=NUM_CLASSES)\ncheckpoint = torch.load(args.weights, map_location=DEVICE)\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.to(DEVICE).eval()\n\n# Directory where all the images are present.\nDIR_TEST = args.input\ntest_images = glob.glob(f\"{DIR_TEST}/*.jpg\")\nprint(f\"Test instances: {len(test_images)}\")\n\nframe_count = 0 # To count total frames.\ntotal_fps = 0 # To get the final frames per second.\n\ndef infer_transforms(image):\n    # Define the torchvision image transforms.\n    transform = transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.ToTensor(),\n    ])\n    return transform(image)\n\nfor i in range(len(test_images)):\n    # Get the image file name for saving output later on.\n    image_name = test_images[i].split(os.path.sep)[-1].split('.')[0]\n    image = cv2.imread(test_images[i])\n    orig_image = image.copy()\n    if args.imgsz is not None:\n        image = cv2.resize(image, (args.imgsz, args.imgsz))\n    print(image.shape)\n    # BGR to RGB.\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    # Apply transforms\n    image_input = infer_transforms(image)\n    # Add batch dimension.\n    image_input = torch.unsqueeze(image_input, 0)\n    start_time = time.time()\n    # Predictions\n    with torch.no_grad():\n        outputs = model(image_input.to(DEVICE))\n    end_time = time.time()\n\n    # Get the current fps.\n    fps = 1 / (end_time - start_time)\n    # Total FPS till current frame.\n    total_fps += fps\n    frame_count += 1\n\n    # Load all detection to CPU for further operations.\n    outputs = [{k: v.to('cpu') for k, v in t.items()} for t in outputs]\n    # Carry further only if there are detected boxes.\n    if len(outputs[0]['boxes']) != 0:\n        boxes = outputs[0]['boxes'].data.numpy()\n        scores = outputs[0]['scores'].data.numpy()\n        # Filter out boxes according to `detection_threshold`.\n        boxes = boxes[scores >= args.threshold].astype(np.int32)\n        draw_boxes = boxes.copy()\n        # Get all the predicited class names.\n        pred_classes = [CLASSES[i] for i in outputs[0]['labels'].cpu().numpy()]\n        \n        # Draw the bounding boxes and write the class name on top of it.\n        for j, box in enumerate(draw_boxes):\n            class_name = pred_classes[j]\n            color = COLORS[CLASSES.index(class_name)]\n            # Recale boxes.\n            xmin = int((box[0] / image.shape[1]) * orig_image.shape[1])\n            ymin = int((box[1] / image.shape[0]) * orig_image.shape[0])\n            xmax = int((box[2] / image.shape[1]) * orig_image.shape[1])\n            ymax = int((box[3] / image.shape[0]) * orig_image.shape[0])\n            cv2.rectangle(orig_image,\n                        (xmin, ymin),\n                        (xmax, ymax),\n                        color[::-1], \n                        3)\n            cv2.putText(orig_image, \n                        class_name, \n                        (xmin, ymin-5),\n                        cv2.FONT_HERSHEY_SIMPLEX, \n                        0.8, \n                        color[::-1], \n                        2, \n                        lineType=cv2.LINE_AA)\n\n        cv2.imshow('Prediction', orig_image)\n        cv2.waitKey(1)\n        cv2.imwrite(f\"inference_outputs/images/{image_name}.jpg\", orig_image)\n    print(f\"Image {i+1} done...\")\n    print('-'*50)\n\nprint('TEST PREDICTIONS COMPLETE')\ncv2.destroyAllWindows()\n# Calculate and print the average FPS.\navg_fps = total_fps / frame_count\nprint(f\"Average FPS: {avg_fps:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:49:07.180093Z","iopub.execute_input":"2023-10-06T16:49:07.180474Z","iopub.status.idle":"2023-10-06T16:49:07.202203Z","shell.execute_reply.started":"2023-10-06T16:49:07.180445Z","shell.execute_reply":"2023-10-06T16:49:07.201077Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"usage: ipykernel_launcher.py [-h] [--weights WEIGHTS] -i INPUT [--imgsz IMGSZ]\n                             [--threshold THRESHOLD]\nipykernel_launcher.py: error: the following arguments are required: -i/--input\n","output_type":"stream"},{"traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"],"ename":"SystemExit","evalue":"2","output_type":"error"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}